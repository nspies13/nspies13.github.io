[
  {
    "objectID": "algorithmic_fairness.html",
    "href": "algorithmic_fairness.html",
    "title": "Algorithmic Fairness",
    "section": "",
    "text": "A crucial aspect of deploying machine learning (ML) models in clinical laboratories is ensuring that they achieve their desired goals without introducing or exacerbating inequity in healthcare delivery. We will examine these vulnerabilities through the lens of fairness concepts and their metrics during the model validation process. Azimi and Zaydman1 have provided a more comprehensive overview of the key considerations for laboratory medicine.\n\n\n\n\n\n\n\nConcepts of algorithmic fairness, adapted from Azimi and Zaydman.\n\n\n\n\n\nPredictive performance can be contextualized within three concepts of group fairness;\n\nDemographic parity, where model flag rates are identical across subgroups,\nEqualized odds, where sensitivity and specificity are identical across subgroups,\nPredictive parity, where PPV and NPV are identical across subgroups.\n\n\n\n\n\n\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n## Load models\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nmodel_retrospective &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631491\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\npredictors &lt;- model_retrospective |&gt; extract_recipe() |&gt; pluck(\"term_info\") |&gt; dplyr::filter(role == \"predictor\") |&gt; pluck(\"variable\")\n\n## Load validation data\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\") |&gt; select(any_of(predictors))\n\n## Add ground-truth labels via retrospective model\nvalidation_with_ground_truth &lt;- augment(model_retrospective, validation |&gt; drop_na(any_of(predictors))) |&gt; mutate(truth = factor(.pred_class, labels = c(\"Negative\", \"Positive\"))) |&gt; select(-matches(\"pred\"))\n\n## Randomly assign two-thirds of the positive classes to females\nvalidation_with_sex_labels &lt;- \n  validation_with_ground_truth |&gt; \n    mutate(sex = ifelse(truth == \"Positive\", sample(c(\"Male\", \"Female\"), size = n(), replace = TRUE, prob = c(1, 2)), NA_character_))\n\n## Assign equal proportions of the negative classes to each sex\nvalidation_with_sex_labels &lt;- \n  validation_with_sex_labels |&gt; \n    mutate(sex = ifelse(truth == \"Negative\", sample(c(\"Male\", \"Female\"), size = n(), replace = TRUE), sex))\n\n\n\n\nLet’s explore how we will calculate these metrics using our normal saline predictor in R.\nFirst, we’ll add a set of simulated labels to our validation set.\nWe’ll make contamination twice as common in females as in males.\n\n\n\n\n\n\nContaminated Results by Sex\n\n\n\nFemale\nMale\n\n\n\n\nNegative\n136807\n137511\n\n\nPositive\n1196\n590\n\n\n\n\n\n\n\n\n\nNext, we’ll calculate a set of performance metrics, including sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and flag rate, for each sex (left).\nThen, we’ll compare these metrics across the demographic groups using the fairness concepts described above (right).\n\n\nShow the Code\n## Apply real-time predictions \nvalidation_realtime_preds &lt;- augment(model_realtime, validation_with_sex_labels) |&gt; mutate(pred = factor(.pred_class, labels = c(\"Negative\", \"Positive\")))\n\n## Calculate group-wise performance metrics\nmetrics_to_calculate &lt;- metric_set(sens, spec, ppv, npv, detection_prevalence)\ngroupwise_metrics &lt;- \n  validation_realtime_preds |&gt; \n    group_by(sex) |&gt; \n    metrics_to_calculate(truth = truth, estimate = pred, event_level = \"second\") |&gt; \n    transmute(Metric = str_to_upper(.metric), Value = .estimate, Sex = sex) |&gt; \n    mutate(Metric = ifelse(Metric == \"DETECTION_PREVALENCE\", \"FLAG RATE\", Metric))\n\n## Output metrics comparison\nmetric_table &lt;- groupwise_metrics |&gt; pivot_wider(id_cols = Sex, names_from = Metric, values_from = Value) |&gt; knitr::kable(digits = 3)\n\n## Add predictive parity to yardstick's prebuilt functions\ndiff_range &lt;- function(x) {diff(range(x$.estimate))}\npredictive_parity &lt;- new_groupwise_metric(ppv, name = \"predictive_parity\", aggregate = diff_range)\n\n## Define fairness metrics to calculate\nfairness_metrics &lt;- metric_set(demographic_parity(by = sex), equalized_odds(by = sex), predictive_parity(by = sex))\n\n## Calculate fairness metrics \nvalidation_fairness &lt;- \n  validation_realtime_preds |&gt; \n    fairness_metrics(truth = truth, estimate = pred, event_level = \"second\") |&gt; \n    transmute(Metric = str_to_upper(.metric), `Metric Difference` = .estimate) \n\n## Make fairness table\nfairness_table &lt;- validation_fairness |&gt; knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\nSex\nSENS\nSPEC\nPPV\nNPV\nFLAG RATE\n\n\n\n\nFemale\n0.789\n0.996\n0.628\n0.998\n0.011\n\n\nMale\n0.802\n0.996\n0.435\n0.999\n0.008\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nMetric Difference\n\n\n\n\nDEMOGRAPHIC_PARITY\n0.003\n\n\nEQUALIZED_ODDS\n0.012\n\n\nPREDICTIVE_PARITY\n0.193\n\n\n\n\n\n\n\n\n\n\nDifferences in incidence of contamination across the demographic groups leads to:\n\nDiscrepant positive predictive values and poor predictive parity.\n\nClass imbalance, with positive cases being quite rare, leads to:\n\nLarge relative, but small absolute, differences in flag rate.\n\nGiven the random nature of the assigned labels:\n\nSensitivity and specificity are nearly identical across groups.\n\n\n\n\n\n\n\nKey Takeaway:\nPerformance assessment should incorporate concepts of algorithmic fairness to protect against the introduction or exacerbation of inequity."
  },
  {
    "objectID": "algorithmic_fairness.html#fairness-concepts",
    "href": "algorithmic_fairness.html#fairness-concepts",
    "title": "Algorithmic Fairness",
    "section": "",
    "text": "Concepts of algorithmic fairness, adapted from Azimi and Zaydman.\n\n\n\n\n\nPredictive performance can be contextualized within three concepts of group fairness;\n\nDemographic parity, where model flag rates are identical across subgroups,\nEqualized odds, where sensitivity and specificity are identical across subgroups,\nPredictive parity, where PPV and NPV are identical across subgroups."
  },
  {
    "objectID": "algorithmic_fairness.html#assessing-fairness-in-contamination-predictions",
    "href": "algorithmic_fairness.html#assessing-fairness-in-contamination-predictions",
    "title": "Algorithmic Fairness",
    "section": "",
    "text": "Show the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n## Load models\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nmodel_retrospective &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631491\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\npredictors &lt;- model_retrospective |&gt; extract_recipe() |&gt; pluck(\"term_info\") |&gt; dplyr::filter(role == \"predictor\") |&gt; pluck(\"variable\")\n\n## Load validation data\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\") |&gt; select(any_of(predictors))\n\n## Add ground-truth labels via retrospective model\nvalidation_with_ground_truth &lt;- augment(model_retrospective, validation |&gt; drop_na(any_of(predictors))) |&gt; mutate(truth = factor(.pred_class, labels = c(\"Negative\", \"Positive\"))) |&gt; select(-matches(\"pred\"))\n\n## Randomly assign two-thirds of the positive classes to females\nvalidation_with_sex_labels &lt;- \n  validation_with_ground_truth |&gt; \n    mutate(sex = ifelse(truth == \"Positive\", sample(c(\"Male\", \"Female\"), size = n(), replace = TRUE, prob = c(1, 2)), NA_character_))\n\n## Assign equal proportions of the negative classes to each sex\nvalidation_with_sex_labels &lt;- \n  validation_with_sex_labels |&gt; \n    mutate(sex = ifelse(truth == \"Negative\", sample(c(\"Male\", \"Female\"), size = n(), replace = TRUE), sex))\n\n\n\n\nLet’s explore how we will calculate these metrics using our normal saline predictor in R.\nFirst, we’ll add a set of simulated labels to our validation set.\nWe’ll make contamination twice as common in females as in males.\n\n\n\n\n\n\nContaminated Results by Sex\n\n\n\nFemale\nMale\n\n\n\n\nNegative\n136807\n137511\n\n\nPositive\n1196\n590\n\n\n\n\n\n\n\n\n\nNext, we’ll calculate a set of performance metrics, including sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and flag rate, for each sex (left).\nThen, we’ll compare these metrics across the demographic groups using the fairness concepts described above (right).\n\n\nShow the Code\n## Apply real-time predictions \nvalidation_realtime_preds &lt;- augment(model_realtime, validation_with_sex_labels) |&gt; mutate(pred = factor(.pred_class, labels = c(\"Negative\", \"Positive\")))\n\n## Calculate group-wise performance metrics\nmetrics_to_calculate &lt;- metric_set(sens, spec, ppv, npv, detection_prevalence)\ngroupwise_metrics &lt;- \n  validation_realtime_preds |&gt; \n    group_by(sex) |&gt; \n    metrics_to_calculate(truth = truth, estimate = pred, event_level = \"second\") |&gt; \n    transmute(Metric = str_to_upper(.metric), Value = .estimate, Sex = sex) |&gt; \n    mutate(Metric = ifelse(Metric == \"DETECTION_PREVALENCE\", \"FLAG RATE\", Metric))\n\n## Output metrics comparison\nmetric_table &lt;- groupwise_metrics |&gt; pivot_wider(id_cols = Sex, names_from = Metric, values_from = Value) |&gt; knitr::kable(digits = 3)\n\n## Add predictive parity to yardstick's prebuilt functions\ndiff_range &lt;- function(x) {diff(range(x$.estimate))}\npredictive_parity &lt;- new_groupwise_metric(ppv, name = \"predictive_parity\", aggregate = diff_range)\n\n## Define fairness metrics to calculate\nfairness_metrics &lt;- metric_set(demographic_parity(by = sex), equalized_odds(by = sex), predictive_parity(by = sex))\n\n## Calculate fairness metrics \nvalidation_fairness &lt;- \n  validation_realtime_preds |&gt; \n    fairness_metrics(truth = truth, estimate = pred, event_level = \"second\") |&gt; \n    transmute(Metric = str_to_upper(.metric), `Metric Difference` = .estimate) \n\n## Make fairness table\nfairness_table &lt;- validation_fairness |&gt; knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\nSex\nSENS\nSPEC\nPPV\nNPV\nFLAG RATE\n\n\n\n\nFemale\n0.789\n0.996\n0.628\n0.998\n0.011\n\n\nMale\n0.802\n0.996\n0.435\n0.999\n0.008\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nMetric Difference\n\n\n\n\nDEMOGRAPHIC_PARITY\n0.003\n\n\nEQUALIZED_ODDS\n0.012\n\n\nPREDICTIVE_PARITY\n0.193\n\n\n\n\n\n\n\n\n\n\nDifferences in incidence of contamination across the demographic groups leads to:\n\nDiscrepant positive predictive values and poor predictive parity.\n\nClass imbalance, with positive cases being quite rare, leads to:\n\nLarge relative, but small absolute, differences in flag rate.\n\nGiven the random nature of the assigned labels:\n\nSensitivity and specificity are nearly identical across groups.\n\n\n\n\n\n\n\nKey Takeaway:\nPerformance assessment should incorporate concepts of algorithmic fairness to protect against the introduction or exacerbation of inequity."
  },
  {
    "objectID": "metric_selection.html",
    "href": "metric_selection.html",
    "title": "Selecting Effective Performance Metrics",
    "section": "",
    "text": "Selecting Effective Performance Metrics\n\nMany clinical applications of ML involve the prediction of rare events. In these cases, the classic metrics of discriminatory performance (e.g. accuracy, sensitivity, specificity, and the area under the ROC curve) may provide overly optimistic estimates of real-world performance.\nLet’s explore the effect of class imbalance on various performance metrics by simulating a model’s predictions across varying degrees of imbalance.\n\n\nMeasuring Performance on Imbalanced Classes\n\n\n\n\n\nRandomly sample 100,000 results from the negative class.\nAdd to it a random sample of positives at varying class imbalance.\nCalculate classic performance metrics and build ROC/PR curves.\nCompare the effect of class imbalance on each metric.\n\n\n\n\n\n\n\n\n\n\n\nAs our positive class – the event we are trying to predict – becomes rarer and rarer, we will need to adjust the performance metrics we use to evaluate our machine learning pipeline.\n\n\nShow the Code\n# Define prevalence levels for class imbalance\nprevalences &lt;- c(0.001, 0.01, 0.1, 0.5)\ntmp &lt;- tibble()\n\n# Build multiple datasets with varying class imbalance\nfor (prevalence in prevalences) {\n  \n  n_pos &lt;- 100000 * prevalence \n  n_neg &lt;- 100000 * (1 - prevalence)\n  pos &lt;- tibble(result = rnorm(n_pos, mean = 0.65, sd = 0.15), label = \"Positive\", prevalence = prevalence)\n  neg &lt;- tibble(result = rnorm(n_neg, mean = 0.35, sd = 0.15), label = \"Negative\", prevalence = prevalence)\n  tmp &lt;- bind_rows(tmp, pos, neg)\n  \n}\n\n# Assign class labels based on predicted probability for each dataset\ngg_input &lt;- \n  tmp |&gt; \n    mutate(label = factor(label, levels = c(\"Positive\", \"Negative\")), \n           estimate = factor(ifelse(result &gt; 0.5, \"Positive\", \"Negative\"), levels = c(\"Positive\", \"Negative\")),\n           prevalence = factor(prevalence))\n\n# Calculate performance metrics for each dataset\ngg_metric_input &lt;- \n  gg_input |&gt;\n  group_by(prevalence) |&gt;\n  summarise(\n    Accuracy = accuracy_vec(label, estimate),\n    Sens = sens_vec(label, estimate),\n    Spec = spec_vec(label, estimate),\n    PPV = ppv_vec(label, estimate),\n    NPV = npv_vec(label, estimate),\n    MCC = mcc_vec(label, estimate),\n    auROC = roc_auc_vec(label, result), \n    auPRC = pr_auc_vec(label, result)) |&gt; \n  pivot_longer(cols = -prevalence, names_to = \"Metric\", values_to = \"Value\") |&gt; \n  mutate(Value = round(Value, 3), Metric = factor(Metric, levels = c(\"auROC\", \"Accuracy\", \"Sens\", \"Spec\", \"NPV\", \"PPV\", \"MCC\", \"auPRC\")))\n\n# Build ROC curves for each dataset\ngg_roc_input &lt;- \n  gg_input |&gt;\n  group_by(prevalence) |&gt;\n  roc_curve(label, result)\n\n# Build PR curves for each dataset\ngg_pr_input &lt;- \n  gg_input |&gt;\n  group_by(prevalence) |&gt;\n  pr_curve(label, result)\n\n# Plot ROC curves for each dataset\ngg_rocs &lt;- \n  ggplot(gg_roc_input, aes(1-specificity, sensitivity, color = prevalence)) +\n    geom_path(linewidth = 2) +\n    geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"grey50\") +\n    scico::scale_color_scico_d(palette = \"lipari\", begin = 0.1, end = 0.9, labels = c(\"Extremely Imbalanced\", \"Imbalanced\", \"Mildly Imbalanced\", \"Balanced\")) +\n    guides(color = guide_legend(title = \"Class Imbalance\", reverse = T)) +\n    labs(x = \"1 - Specificity\", y = \"Sensitivity\", title = \"ROC Curves\") + \n    theme(axis.text = element_blank(), legend.position = c(0.8, 0.3), legend.background = element_blank())\n  \n# Plot PR curves for each dataset\ngg_prs &lt;- \n  ggplot(gg_pr_input, aes(recall, precision, color = prevalence)) +\n    geom_path(linewidth = 2) +\n    scico::scale_color_scico_d(palette = \"lipari\", begin = 0.1, end = 0.9) +\n    labs(x = \"Recall (Sensitivity)\", y = \"Precision (PPV)\", title = \"PR Curves\") + \n    theme(axis.text = element_blank(), legend.position = \"none\")\n\n# Plot the ROC and PR curves side-by-side and save them to files of each format\ngg_imbalance_curves &lt;- ggpubr::ggarrange(gg_rocs, gg_prs, nrow = 1, ncol = 2)\nggsave(\"../../figures/imbalanced_curves.png\", gg_imbalance_curves, width = 10, height = 4, dpi = 600)\nggsave(\"../../figures/imbalanced_curves.pdf\", gg_imbalance_curves, width = 10, height = 4)\nggsave(\"../../figures/imbalanced_curves.svg\", gg_imbalance_curves, width = 10, height = 4)\n\n# Plot the performance metrics for each dataset\ngg_metrics &lt;-\n  ggplot(gg_metric_input, aes(Metric, Value, fill = fct_rev(prevalence))) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scico::scale_fill_scico_d(palette = \"lipari\", begin = 0.9, end = 0.1) +\n    scale_y_continuous(name = \"Metric Value\", limits = c(0, 1), breaks = c(0, 1)) +\n    scale_x_discrete(name = NULL) +\n    ggtitle(\"Binary Metrics\") + \n    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \"none\", axis.text.x.bottom = element_text(angle = 0, face = \"bold\", hjust = 0.5))\n\n# Plot the curves and metrics side-by-side and save them to files of each format\ngg_imbalance_metrics_combined &lt;- \n  ggpubr::ggarrange(\n    gg_imbalance_curves, \n    gg_metrics,\n      ncol = 1, nrow = 2, heights = c(0.6, 0.4))\n\nggsave(\"../../figures/imbalanced_metrics_combined.png\", gg_imbalance_metrics_combined, width = 10, height = 6, dpi = 600, bg = NULL)\nggsave(\"../../figures/imbalanced_metrics_combined.pdf\", gg_imbalance_metrics_combined, width = 10, height = 6, dpi = 600, bg = NULL)\nggsave(\"../../figures/imbalanced_metrics_combined.svg\", gg_imbalance_metrics_combined, width = 10, height = 6, dpi = 600, bg = NULL)\n\n# Display the combined plot\ngg_imbalance_metrics_combined\n\n\n\n\n\nFigure 1: The effect of class imbalance on various performance metrics. Gray lines and bars highlight a balanced classification task, while red represents a task where negatives far outnumber positives.\n\n\n\n\nHere, we see a stark contrast between metrics that are sensitive to class imbalance (e.g. PPV/NPV, MCC, and the Precision-Recall curve) and those that are not (e.g. Sensitivity, Specificity, Accuracy, and the ROC curve).\n\n\n\n\n\n\nKey Takeaway:\nWhen predicting rare events, use Precision-Recall Curves, Positive Predictive Value, and the Matthews Correlation Coefficient (MCC) to better assess clinical utility in real-world applications."
  },
  {
    "objectID": "decision_boundaries.html",
    "href": "decision_boundaries.html",
    "title": "Optimizing Decision Boundaries",
    "section": "",
    "text": "The output of most ML models is a continuous “probability” score, which must be converted into a class label by applying some kind of decision boundaries. This can be achieved in a number of ways, ranging from simple to quite complex. As an illustrative example, we will use the normal saline prediction task, with the outputs from the real-time model as the predictions, and the outputs from the retrospective model as the ground truth.\n\n\nA relatively simple approach to setting a decision boundary is to visually inspect the distribution of predicted probabilities for each class, and draw a separator where you deem the balance of sensitivity and specificity is appropriate. Below, with R, we will visualize the distributions as a standard density plot.\n\n\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n## Load models\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nmodel_retrospective &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631491\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\npredictors &lt;- model_retrospective |&gt; extract_recipe() |&gt; pluck(\"term_info\") |&gt; dplyr::filter(role == \"predictor\") |&gt; pluck(\"variable\")\n\n## Load validation data\noptions(timeout=300)\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\") |&gt; select(any_of(predictors))\nvalidation_no_NA &lt;- validation |&gt; drop_na(matches(\"prior|post|potassium\"))\n\n## Make predictions using real-time model\nvalidation_predictions &lt;- \n  bind_cols(\n    validation_no_NA,\n    predicted_probability = predict(model_realtime, new_data = validation_no_NA, type = \"prob\") |&gt; pluck(\".pred_1\"),\n    ground_truth = factor(predict(model_retrospective, new_data = validation_no_NA, type = \"class\") |&gt; pluck(\".pred_class\"), labels = c(\"Negative\", \"Positive\"))\n    ) |&gt; \n  mutate(predicted_class = factor(ifelse(predicted_probability &gt; 0.5, \"Positive\", \"Negative\")))\n\n## Calculate performance metrics at a threshold of 0.5\nsens &lt;- sensitivity(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\nspec &lt;- specificity(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\npos_pred_value &lt;- ppv(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\nneg_pred_value &lt;- npv(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\nmatthews_corr_coef &lt;- mcc(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\n                                    \n## Visualizing the differences in the distributions between positive and negative classes. \ngg_dist &lt;- \n  ggplot(validation_predictions, aes(x = predicted_probability, fill = ground_truth)) +\n    geom_density(bw = 0.1) +\n    geomtextpath::geom_textdensity(aes(label = ground_truth, color = ground_truth, hjust = ground_truth), \n                                   linewidth = 1.25, alpha = 0.75, bw = 0.1, \n                                   vjust = -0.5, fontface = \"bold\", size = 8) +\n    scale_y_continuous(name = \"Proportion\", expand = c(0, 0), labels = NULL) +\n    scale_x_continuous(name = \"Predicted Probability\", expand = c(0.01, 0.01), breaks = c(0, 0.5, 1)) +\n    scale_discrete_manual(aes = \"hjust\", values = c(0.1, 0.95)) +\n    scico::scale_fill_scico_d(palette = \"lipari\", begin = 0.1, end = 0.5) +\n    scico::scale_color_scico_d(palette = \"lipari\", begin = 0.1, end = 0.5) +\n    labs(x = \"Predicted Probability\", y = \"Density\", fill = \"Ground Truth\") +\n    ggtitle(\"Distribution of Predicted Probabilities by Class\") +\n    theme(legend.position = \"none\", axis.line.y.left = element_blank())\n\ngg_dist +   \n  geom_vline(xintercept = 0.5, linetype = \"dashed\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.52, y = 3, label = \"Threshold\", fontface = \"bold\", angle = -90, size = 6)\n\n\n\n\n\n\n\n\n\n\nSens: 0.793\nSpec: 0.996\nPPV: 0.547\nNPV: 0.999\nMCC: 0.656\n\n\n\n\n\n\nThe Receiver Operating Characteristic (ROC) curve has long been a staple in the evaluation of binary classifiers, and is another useful tool for setting a decision boundary. When using an ROC curve to establish a decision boundary, it is common to calculate the point at which the sum of sensitivity and specificity are maximized. This can be achieved by calculating Youden’s J index across multiple thresholds.\n\n\n\n\nShow the Code\n## Calculate Sensitivity and Specificity Across All Thresholds\nroc &lt;- \n  roc_curve(validation_predictions, predicted_probability, truth = ground_truth, event_level = \"second\") |&gt; \n  mutate(youden = sensitivity + specificity - 1) # Add Youden's J at each threshold\n\n## Find the Threshold that maximizes Youden's J\nthreshold &lt;- roc[which.max(roc$youden), ]\n\n## Plot ROC Curve\nggplot(roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_step(size = 1.25) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\", size = 1) +\n  annotate(\"segment\", x = 1 - threshold[[\"specificity\"]], xend = 1 - threshold[[\"specificity\"]], y = threshold[[\"sensitivity\"]], yend = threshold[[\".threshold\"]], color = \"grey80\", linewidth = 1.25) + \n  labs(x = \"1 - Specificity\", y = \"Sensitivity\") +\n  annotate(\"text\", x = 1 - threshold[[\"specificity\"]] + 0.02, y = 0.6, label = \"Youden's J\", fontface = \"bold.italic\", color = \"grey80\", size = 6, hjust = 0) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\") +\n  ggtitle(\"ROC Curve\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition:\nJ = Sensitivity + Specificity - 1\n\n\n\n\n\n\n\n\n\nKey Consideration:\nMaximizing Youden’s J will provide the optimal threshold iff sensitivity and specificity are equally important.\n\n\n\n\n\n\n\n\n\nMaximizing Youden’s J index works well if we care equally about sensitivity and specificity, but real-life clinical scenarios in which a false positive and a false negative are equally disruptive are rare. Additionally, the section on Measuring Performance highlighted that metrics like sensitivity and specificity can be misleading in the setting of a class imbalance. Let’s explore how we might use metrics like the NPV, PPV, and MCC to set a decision boundary. For this, we’ll use the {probably}1 package.\n\n\nShow the Code\nlibrary(probably)\n\n# Define our thresholds to test\nthresholds &lt;- seq(0.01, 0.99, by = 0.01)\n  \n# Define our metrics of interest\ndecision_metrics &lt;- metric_set(ppv, mcc, j_index)\n\n# Calculate each metric across each threshold\ndecision_curves &lt;- \n  validation_predictions |&gt; \n  threshold_perf(truth = ground_truth, estimate = predicted_probability, \n                 metrics = decision_metrics, \n                 thresholds = thresholds, \n                 event_level = \"second\")\nmax_mcc &lt;- decision_curves |&gt; dplyr::filter(.metric == \"mcc\") |&gt; arrange(desc(.estimate)) |&gt; slice_head(n = 1)\nmax_J &lt;- decision_curves |&gt; dplyr::filter(.metric == \"j_index\") |&gt; arrange(desc(.estimate)) |&gt; slice_head(n = 1)\n\n# Plot Results\nggplot(decision_curves, aes(x = .threshold, y = .estimate, color = .metric)) + \n  geomtextpath::geom_textline(aes(label = str_to_upper(.metric)), linewidth = 1.5, fontface = \"bold\", size = 8, hjust = 0.25) + \n  geom_vline(xintercept = max_mcc[[\".threshold\"]], linetype = \"dashed\") +\n  geom_text(data = max_mcc, x = max_mcc[[\".threshold\"]] - 0.02, y = 0.1, hjust = 1, fontface = \"bold\", label = glue::glue(\"Max MCC at a cut-off of \", max_mcc[[\".threshold\"]])) + \n  geom_point(data = max_mcc, size = 6, aes(color = \"mcc\")) + \n  geom_vline(xintercept = max_J[[\".threshold\"]], linetype = \"dashed\") +\n  geom_text(data = max_J, x = max_J[[\".threshold\"]] + 0.02, y = 0.1, hjust = 0, fontface = \"bold\", label = glue::glue(\"Max Youden's J at a cut-off of \", max_J[[\".threshold\"]])) + \n  geom_point(data = max_J, size = 6, aes(color = \"j_index\")) + \n  scico::scale_color_scico_d(palette = \"lipari\", begin = 0.1, end = 0.9) +\n  scale_x_continuous(name = \"Prediction Threshold\", breaks = c(0, 0.5, 1)) + \n  scale_y_continuous(name = \"Metric Value\", breaks = c(0, 0.5, 1)) + \n  ggtitle(\"Performance Metrics Across a Range of Thresholds\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nUsing the more imbalance-sensitive metrics, we can see that the threshold that maximizes the Matthews Correlation Coefficient (MCC) is substantially different from the threshold that maximizes Youden’s J index.\n\n\n\nIn some clinical scenarios, where the cost of a false positive and/or a false negative prediction is high, it may be beneficial to define an “equivocal zone” where the model’s continuous output is not converted into a class prediction. This is particularly helpful when planning for fully automated implementations, where extreme confidence is needed before deciding to forgo a human review step.\n\n\nShow the Code\ngg_dist_with_equiv &lt;-\n  gg_dist + \n    geom_vline(xintercept = c(0.25, 0.75), linetype = \"dashed\") + \n    annotate(\"rect\", xmin = 0.25, xmax = 0.75, ymin = 0, ymax = 5, fill = \"gray70\", alpha = 0.5) + \n    annotate(\"text\", x = 0.5, y = 2.5, label = \"Equivocal\", hjust = 0.5, size = 6, fontface = \"bold\") + \n    ## Add a segment with an arrow on either side\n    annotate(\"segment\", x = 0.6, xend = 0.73, y = 2.5, yend = 2.5, arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = 0.4, xend = 0.27, y = 2.5, yend = 2.5, arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\"))) + \n    ggtitle(\"Decision Boundaries with an Equivocal Zone\")\ngg_dist_with_equiv\n\n\n\n\n\n\n\n\n\nFor example, let’s suppose that we can tolerate not making predicitons on a subset of results to improve our PPV and NPV. We can again use the {probably}1 package to set an equivocal zone between these thresholds, shown in code below.\n\n\n\n\nShow the Code\n# Define our equivocal zone\npredictions_with_equivocal_zone &lt;-\n  validation_predictions |&gt; \n    mutate(\n      .pred = make_two_class_pred(\n        estimate = 1 - predicted_probability, \n        levels = levels(ground_truth),\n        threshold = 0.4,\n        buffer = 0.25\n      )\n    )\n\n# Calculate the reportable rate and performance metrics\nclass_metrics &lt;- metric_set(ppv, npv, mcc)\n\nperformance_without_equivocal &lt;- class_metrics(validation_predictions, truth = ground_truth, estimate = predicted_class, event_level = \"second\") |&gt; mutate(type = \"Standard\", reportable_rate = 1)\nperformance_with_equivocal &lt;- class_metrics(predictions_with_equivocal_zone, truth = ground_truth, estimate = .pred, event_level = \"second\") |&gt; mutate(type = \"With Equivocal Zone\", reportable_rate = round(reportable_rate(predictions_with_equivocal_zone$.pred), digits = 3))\n\n# Combine the results\nperformance &lt;- \n  bind_rows(performance_without_equivocal, performance_with_equivocal) |&gt; \n  pivot_wider(names_from = .metric, values_from = c(.estimate)) |&gt; \n  select(-.estimator)\n\n# Print the results\nperformance |&gt; knitr::kable(digits = 3)\n\n\n\n\n\ntype\nreportable_rate\nppv\nnpv\nmcc\n\n\n\n\nStandard\n1.000\n0.547\n0.999\n0.656\n\n\nWith Equivocal Zone\n0.993\n0.789\n0.999\n0.791\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaway\nThe incorporation of an Equivocal Zone can improve performance at the cost of the proportion of results with predictions."
  },
  {
    "objectID": "decision_boundaries.html#visual-inspection",
    "href": "decision_boundaries.html#visual-inspection",
    "title": "Optimizing Decision Boundaries",
    "section": "",
    "text": "A relatively simple approach to setting a decision boundary is to visually inspect the distribution of predicted probabilities for each class, and draw a separator where you deem the balance of sensitivity and specificity is appropriate. Below, with R, we will visualize the distributions as a standard density plot.\n\n\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n## Load models\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nmodel_retrospective &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631491\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\npredictors &lt;- model_retrospective |&gt; extract_recipe() |&gt; pluck(\"term_info\") |&gt; dplyr::filter(role == \"predictor\") |&gt; pluck(\"variable\")\n\n## Load validation data\noptions(timeout=300)\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\") |&gt; select(any_of(predictors))\nvalidation_no_NA &lt;- validation |&gt; drop_na(matches(\"prior|post|potassium\"))\n\n## Make predictions using real-time model\nvalidation_predictions &lt;- \n  bind_cols(\n    validation_no_NA,\n    predicted_probability = predict(model_realtime, new_data = validation_no_NA, type = \"prob\") |&gt; pluck(\".pred_1\"),\n    ground_truth = factor(predict(model_retrospective, new_data = validation_no_NA, type = \"class\") |&gt; pluck(\".pred_class\"), labels = c(\"Negative\", \"Positive\"))\n    ) |&gt; \n  mutate(predicted_class = factor(ifelse(predicted_probability &gt; 0.5, \"Positive\", \"Negative\")))\n\n## Calculate performance metrics at a threshold of 0.5\nsens &lt;- sensitivity(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\nspec &lt;- specificity(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\npos_pred_value &lt;- ppv(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\nneg_pred_value &lt;- npv(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\nmatthews_corr_coef &lt;- mcc(validation_predictions, estimate = predicted_class, truth = ground_truth, event_level = \"second\")\n                                    \n## Visualizing the differences in the distributions between positive and negative classes. \ngg_dist &lt;- \n  ggplot(validation_predictions, aes(x = predicted_probability, fill = ground_truth)) +\n    geom_density(bw = 0.1) +\n    geomtextpath::geom_textdensity(aes(label = ground_truth, color = ground_truth, hjust = ground_truth), \n                                   linewidth = 1.25, alpha = 0.75, bw = 0.1, \n                                   vjust = -0.5, fontface = \"bold\", size = 8) +\n    scale_y_continuous(name = \"Proportion\", expand = c(0, 0), labels = NULL) +\n    scale_x_continuous(name = \"Predicted Probability\", expand = c(0.01, 0.01), breaks = c(0, 0.5, 1)) +\n    scale_discrete_manual(aes = \"hjust\", values = c(0.1, 0.95)) +\n    scico::scale_fill_scico_d(palette = \"lipari\", begin = 0.1, end = 0.5) +\n    scico::scale_color_scico_d(palette = \"lipari\", begin = 0.1, end = 0.5) +\n    labs(x = \"Predicted Probability\", y = \"Density\", fill = \"Ground Truth\") +\n    ggtitle(\"Distribution of Predicted Probabilities by Class\") +\n    theme(legend.position = \"none\", axis.line.y.left = element_blank())\n\ngg_dist +   \n  geom_vline(xintercept = 0.5, linetype = \"dashed\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.52, y = 3, label = \"Threshold\", fontface = \"bold\", angle = -90, size = 6)\n\n\n\n\n\n\n\n\n\n\nSens: 0.793\nSpec: 0.996\nPPV: 0.547\nNPV: 0.999\nMCC: 0.656"
  },
  {
    "objectID": "decision_boundaries.html#youdens-j-index-and-the-roc-curve",
    "href": "decision_boundaries.html#youdens-j-index-and-the-roc-curve",
    "title": "Optimizing Decision Boundaries",
    "section": "",
    "text": "The Receiver Operating Characteristic (ROC) curve has long been a staple in the evaluation of binary classifiers, and is another useful tool for setting a decision boundary. When using an ROC curve to establish a decision boundary, it is common to calculate the point at which the sum of sensitivity and specificity are maximized. This can be achieved by calculating Youden’s J index across multiple thresholds.\n\n\n\n\nShow the Code\n## Calculate Sensitivity and Specificity Across All Thresholds\nroc &lt;- \n  roc_curve(validation_predictions, predicted_probability, truth = ground_truth, event_level = \"second\") |&gt; \n  mutate(youden = sensitivity + specificity - 1) # Add Youden's J at each threshold\n\n## Find the Threshold that maximizes Youden's J\nthreshold &lt;- roc[which.max(roc$youden), ]\n\n## Plot ROC Curve\nggplot(roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_step(size = 1.25) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\", size = 1) +\n  annotate(\"segment\", x = 1 - threshold[[\"specificity\"]], xend = 1 - threshold[[\"specificity\"]], y = threshold[[\"sensitivity\"]], yend = threshold[[\".threshold\"]], color = \"grey80\", linewidth = 1.25) + \n  labs(x = \"1 - Specificity\", y = \"Sensitivity\") +\n  annotate(\"text\", x = 1 - threshold[[\"specificity\"]] + 0.02, y = 0.6, label = \"Youden's J\", fontface = \"bold.italic\", color = \"grey80\", size = 6, hjust = 0) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\") +\n  ggtitle(\"ROC Curve\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition:\nJ = Sensitivity + Specificity - 1\n\n\n\n\n\n\n\n\n\nKey Consideration:\nMaximizing Youden’s J will provide the optimal threshold iff sensitivity and specificity are equally important."
  },
  {
    "objectID": "decision_boundaries.html#decision-boundaries-using-imbalance-sensitive-metrics",
    "href": "decision_boundaries.html#decision-boundaries-using-imbalance-sensitive-metrics",
    "title": "Optimizing Decision Boundaries",
    "section": "",
    "text": "Maximizing Youden’s J index works well if we care equally about sensitivity and specificity, but real-life clinical scenarios in which a false positive and a false negative are equally disruptive are rare. Additionally, the section on Measuring Performance highlighted that metrics like sensitivity and specificity can be misleading in the setting of a class imbalance. Let’s explore how we might use metrics like the NPV, PPV, and MCC to set a decision boundary. For this, we’ll use the {probably}1 package.\n\n\nShow the Code\nlibrary(probably)\n\n# Define our thresholds to test\nthresholds &lt;- seq(0.01, 0.99, by = 0.01)\n  \n# Define our metrics of interest\ndecision_metrics &lt;- metric_set(ppv, mcc, j_index)\n\n# Calculate each metric across each threshold\ndecision_curves &lt;- \n  validation_predictions |&gt; \n  threshold_perf(truth = ground_truth, estimate = predicted_probability, \n                 metrics = decision_metrics, \n                 thresholds = thresholds, \n                 event_level = \"second\")\nmax_mcc &lt;- decision_curves |&gt; dplyr::filter(.metric == \"mcc\") |&gt; arrange(desc(.estimate)) |&gt; slice_head(n = 1)\nmax_J &lt;- decision_curves |&gt; dplyr::filter(.metric == \"j_index\") |&gt; arrange(desc(.estimate)) |&gt; slice_head(n = 1)\n\n# Plot Results\nggplot(decision_curves, aes(x = .threshold, y = .estimate, color = .metric)) + \n  geomtextpath::geom_textline(aes(label = str_to_upper(.metric)), linewidth = 1.5, fontface = \"bold\", size = 8, hjust = 0.25) + \n  geom_vline(xintercept = max_mcc[[\".threshold\"]], linetype = \"dashed\") +\n  geom_text(data = max_mcc, x = max_mcc[[\".threshold\"]] - 0.02, y = 0.1, hjust = 1, fontface = \"bold\", label = glue::glue(\"Max MCC at a cut-off of \", max_mcc[[\".threshold\"]])) + \n  geom_point(data = max_mcc, size = 6, aes(color = \"mcc\")) + \n  geom_vline(xintercept = max_J[[\".threshold\"]], linetype = \"dashed\") +\n  geom_text(data = max_J, x = max_J[[\".threshold\"]] + 0.02, y = 0.1, hjust = 0, fontface = \"bold\", label = glue::glue(\"Max Youden's J at a cut-off of \", max_J[[\".threshold\"]])) + \n  geom_point(data = max_J, size = 6, aes(color = \"j_index\")) + \n  scico::scale_color_scico_d(palette = \"lipari\", begin = 0.1, end = 0.9) +\n  scale_x_continuous(name = \"Prediction Threshold\", breaks = c(0, 0.5, 1)) + \n  scale_y_continuous(name = \"Metric Value\", breaks = c(0, 0.5, 1)) + \n  ggtitle(\"Performance Metrics Across a Range of Thresholds\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nUsing the more imbalance-sensitive metrics, we can see that the threshold that maximizes the Matthews Correlation Coefficient (MCC) is substantially different from the threshold that maximizes Youden’s J index."
  },
  {
    "objectID": "decision_boundaries.html#equivocal-zones-and-the-no-prediction-rate",
    "href": "decision_boundaries.html#equivocal-zones-and-the-no-prediction-rate",
    "title": "Optimizing Decision Boundaries",
    "section": "",
    "text": "In some clinical scenarios, where the cost of a false positive and/or a false negative prediction is high, it may be beneficial to define an “equivocal zone” where the model’s continuous output is not converted into a class prediction. This is particularly helpful when planning for fully automated implementations, where extreme confidence is needed before deciding to forgo a human review step.\n\n\nShow the Code\ngg_dist_with_equiv &lt;-\n  gg_dist + \n    geom_vline(xintercept = c(0.25, 0.75), linetype = \"dashed\") + \n    annotate(\"rect\", xmin = 0.25, xmax = 0.75, ymin = 0, ymax = 5, fill = \"gray70\", alpha = 0.5) + \n    annotate(\"text\", x = 0.5, y = 2.5, label = \"Equivocal\", hjust = 0.5, size = 6, fontface = \"bold\") + \n    ## Add a segment with an arrow on either side\n    annotate(\"segment\", x = 0.6, xend = 0.73, y = 2.5, yend = 2.5, arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = 0.4, xend = 0.27, y = 2.5, yend = 2.5, arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\"))) + \n    ggtitle(\"Decision Boundaries with an Equivocal Zone\")\ngg_dist_with_equiv\n\n\n\n\n\n\n\n\n\nFor example, let’s suppose that we can tolerate not making predicitons on a subset of results to improve our PPV and NPV. We can again use the {probably}1 package to set an equivocal zone between these thresholds, shown in code below.\n\n\n\n\nShow the Code\n# Define our equivocal zone\npredictions_with_equivocal_zone &lt;-\n  validation_predictions |&gt; \n    mutate(\n      .pred = make_two_class_pred(\n        estimate = 1 - predicted_probability, \n        levels = levels(ground_truth),\n        threshold = 0.4,\n        buffer = 0.25\n      )\n    )\n\n# Calculate the reportable rate and performance metrics\nclass_metrics &lt;- metric_set(ppv, npv, mcc)\n\nperformance_without_equivocal &lt;- class_metrics(validation_predictions, truth = ground_truth, estimate = predicted_class, event_level = \"second\") |&gt; mutate(type = \"Standard\", reportable_rate = 1)\nperformance_with_equivocal &lt;- class_metrics(predictions_with_equivocal_zone, truth = ground_truth, estimate = .pred, event_level = \"second\") |&gt; mutate(type = \"With Equivocal Zone\", reportable_rate = round(reportable_rate(predictions_with_equivocal_zone$.pred), digits = 3))\n\n# Combine the results\nperformance &lt;- \n  bind_rows(performance_without_equivocal, performance_with_equivocal) |&gt; \n  pivot_wider(names_from = .metric, values_from = c(.estimate)) |&gt; \n  select(-.estimator)\n\n# Print the results\nperformance |&gt; knitr::kable(digits = 3)\n\n\n\n\n\ntype\nreportable_rate\nppv\nnpv\nmcc\n\n\n\n\nStandard\n1.000\n0.547\n0.999\n0.656\n\n\nWith Equivocal Zone\n0.993\n0.789\n0.999\n0.791\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaway\nThe incorporation of an Equivocal Zone can improve performance at the cost of the proportion of results with predictions."
  },
  {
    "objectID": "explainability.html",
    "href": "explainability.html",
    "title": "Explaining Models and Predictions",
    "section": "",
    "text": "Long labeled as “black boxes,” machine learning models have been criticized for their lack of transparency and interpretability. This lack of transparency can be a significant barrier to the adoption of machine learning models in many applications, and has been highlighted in regulatory guidances1,2,3 as essential components of artificial intelligence-enabled medical devices. In this section, we will discuss some current techniques for explaining both models and predictions.\nExplainability techniques are largely classified into those that explain models, or global explanations, and those that explain individual predictions, or local explanations. Each have their advantages and disadvantages. We will use our normal saline prediction model and a technique called SHapley Additive exPlanations4 (SHAP) as an illustrative use case.\n\n\nEstimating the aggregate impact of each feature on model predictions across a full data set can be vital in identifying abnormal, inequitable, or harmful behaviors. Let’s explore how we can use these tools to evaluate how predicted probabilities changes across varying concentrations of our features.\n\n\nPartial dependence plots (PDPs) show the relationship between a feature and the predicted outcome in a hypothetical or simulated world where we hold all other variables constant, and change only the feature of interest. This allows us to understand the marginal effect of a feature on the predicted outcome. We will use the {iml} package from Molnar’s Interpretable Machine Learning5 to generate PDPs for our model.\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n# Load Model and Data\n## Load models\noptions(timeout=300)\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nrecipe &lt;- model_realtime |&gt; extract_recipe()\npredictors &lt;- recipe$term_info |&gt; dplyr::filter(role == \"predictor\") |&gt; pluck(\"variable\")\n\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\") |&gt; select(any_of(predictors))\nvalidation_with_predictions &lt;- augment(model_realtime, validation |&gt; drop_na(matches(\"delta_prior\"))) |&gt; slice_head(prop = 0.05, by = \".pred_class\")\nvalidation_preprocessed &lt;- bake(recipe, validation_with_predictions) |&gt; bind_cols(validation_with_predictions |&gt; select(matches(\"pred\")))\n\nlibrary(iml)\npredict_wrapper &lt;- function(model, newdata){workflows:::predict.workflow(object = model, new_data = newdata, type = \"prob\")} \n\npredictor &lt;- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |&gt; select(any_of(predictors))), y = validation_with_predictions[[\".pred_1\"]], predict.function = predict_wrapper, type = \"prob\", class = 2)\n  \npdp &lt;- FeatureEffect$new(predictor, feature = \"chloride\", method = \"pdp\")\ngg_pdp &lt;- plot(pdp) + xlab(\"Chloride (mmol/L)\") + scale_y_continuous(name = \"Average Marginal Impact\") + ggtitle(\"Partial Dependence Plot\") + coord_cartesian(xlim = c(80, 140))\n\n\n\n\n\nAccumulated Local Effects (ALE) plots are another global explainability technique that can help us understand the relationship between a feature and the model’s predictions. ALE plots show the average effect of changing a feature while accounting for the effects of other features.\n\n\nShow the Code\npredictor &lt;- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |&gt; select(any_of(predictors))), y = validation_with_predictions[[\".pred_1\"]], predict.function = predict_wrapper, type = \"prob\", class = 2)\n\nale &lt;- FeatureEffect$new(predictor, feature = \"chloride\")\ngg_ale &lt;- plot(ale) + scale_x_continuous(name = \"Chloride (mmol/L)\") + \n    scale_y_continuous(name = \"Average Conditional Impact\") + \n    scico::scale_fill_scico(palette = \"lipari\", begin = 0.1, end = 0.9, name = \"Impact on Prediction\") + \n    coord_cartesian(xlim = c(80, 140)) + \n    ggtitle(\"Accumulated Local Effects Plot\")\n\nggpubr::ggarrange(gg_pdp, gg_ale, ncol = 2, nrow = 1)\n\n\n\n\n\nWhile As we can see, we obtain slightly different answers from each approach, though largely they agree. Higher chloride concentrations lead to high predicted probabilities for contamination by normal saline. Some of the idiosyncrasies of each approach are summarized below.\n\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nPartial Dependence Plots (PDP)\n\nIntuitive interpretation.\nCausal relationship.\n\n\nAssumes independence between features.\nComputationally expensive.\n\n\n\nAccumulated Local Effects (ALE)\n\nCan handle correlated features.\nFast and cheap to calculate.\n\n\nBinning leads to odd results across intervals.\nNaive to heterogeneity across feature effects.\n\n\n\n\n\n\n\n\nLocal explainability refers to the ability to explore the effect of each feature on a model’s prediction for any given set of inputs. For our IV fluid detection example, let’s use the following set of BMP results.\n\n\nShow the Code\n# Pick a random highly positive example\nlocal_example &lt;- validation_with_predictions |&gt; arrange(desc(.pred_1)) |&gt; slice_head(n = 1) \n\n# Rename columns without _delta_prior\ndeltas &lt;- local_example |&gt; select(matches(\"_delta_prior\")) |&gt; rename_all(~str_remove(.x, \"_delta_prior\"))\n\n# Print a table of deltas and results\nexample_table &lt;- as.data.frame(bind_rows(local_example |&gt; select(any_of(predictors)) |&gt; select(-matches(\"_delta_prior\")), local_example |&gt; select(any_of(predictors)) |&gt; select(-matches(\"_delta_prior\")) + deltas))\nrow.names(example_table) &lt;- c(\"Prior\", \"Current\")\n\nknitr::kable(example_table, digits = 2, row.names = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsodium\nchloride\npotassium_plas\nco2_totl\nbun\ncreatinine\ncalcium\nglucose\n\n\n\n\nPrior\n142\n120\n2.8\n16\n12\n0.78\n6.0\n117\n\n\nCurrent\n145\n132\n1.3\n9\n8\n0.49\n3.7\n-23\n\n\n\n\n\n\n\nSHAP values4 have become a staple in tabular model interpretation.\n\n\nShow the Code\n# Load Libraries\nlibrary(shapviz)\n\n# Build SHAP explainer\nshap_local &lt;- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(local_example |&gt; select(any_of(predictors)) %&gt;% bake(recipe, .)), X = local_example)\nshap_local$S &lt;- as.matrix(shap_local$S * -1)\n\n# Plot SHAP Values Locally\nsv_waterfall(shap_local, show_annotation = F) + \n  ggtitle(\"Local Explanation of a Positive Prediction with SHAP\") + \n  scico::scale_fill_scico_d(palette = \"vik\", begin = 0.9, end = 0.1) +\n  theme(plot.title = element_text(size = 18, face = \"bold.italic\"))\n\n\n\n\n\nThe local SHAP values for this example show that the high probability prediction is driven largely by the increase in chloride, the high chloride result, and the decrease in calcium. This aligns with our a priori hypotheses as to what saline-contaminated results should look like, adding confidence in the prediction.\n\n\nWe can also aggregate local SHAP values to understand the global importance of each feature in the model’s predictions in the form of a “beeswarm” plot.\n\n\nShow the Code\n# Build SHAP explainer\nshap &lt;- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(validation_with_predictions |&gt; select(any_of(predictors)) %&gt;% bake(recipe, .)), X = as.data.frame(validation_with_predictions))\n\n\n[20:01:48] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\n\nShow the Code\nshap$S &lt;- as.matrix(shap$S * -1)\n\n# Plot SHAP Values as Beeswarm Plot\ngg_bee &lt;- sv_importance(shap, kind = \"beeswarm\", max_display = 5, alpha = 0.75) + scico::scale_color_scico(palette = \"vik\", breaks = c(0, 1), labels = c(\"Low\", \"High\"), name = \"Feature Value\") + xlab(\"Impact on Prediction\") + theme(legend.position = c(0.85, 0.20), legend.direction = \"horizontal\", legend.title.position = \"top\", axis.text.x.bottom = element_blank())\ngg_bee\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaway:\nML models, and their predictions, can be explained using tools such as SHAP values."
  },
  {
    "objectID": "explainability.html#global-explainability",
    "href": "explainability.html#global-explainability",
    "title": "Explaining Models and Predictions",
    "section": "",
    "text": "Estimating the aggregate impact of each feature on model predictions across a full data set can be vital in identifying abnormal, inequitable, or harmful behaviors. Let’s explore how we can use these tools to evaluate how predicted probabilities changes across varying concentrations of our features.\n\n\nPartial dependence plots (PDPs) show the relationship between a feature and the predicted outcome in a hypothetical or simulated world where we hold all other variables constant, and change only the feature of interest. This allows us to understand the marginal effect of a feature on the predicted outcome. We will use the {iml} package from Molnar’s Interpretable Machine Learning5 to generate PDPs for our model.\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n# Load Model and Data\n## Load models\noptions(timeout=300)\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nrecipe &lt;- model_realtime |&gt; extract_recipe()\npredictors &lt;- recipe$term_info |&gt; dplyr::filter(role == \"predictor\") |&gt; pluck(\"variable\")\n\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\") |&gt; select(any_of(predictors))\nvalidation_with_predictions &lt;- augment(model_realtime, validation |&gt; drop_na(matches(\"delta_prior\"))) |&gt; slice_head(prop = 0.05, by = \".pred_class\")\nvalidation_preprocessed &lt;- bake(recipe, validation_with_predictions) |&gt; bind_cols(validation_with_predictions |&gt; select(matches(\"pred\")))\n\nlibrary(iml)\npredict_wrapper &lt;- function(model, newdata){workflows:::predict.workflow(object = model, new_data = newdata, type = \"prob\")} \n\npredictor &lt;- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |&gt; select(any_of(predictors))), y = validation_with_predictions[[\".pred_1\"]], predict.function = predict_wrapper, type = \"prob\", class = 2)\n  \npdp &lt;- FeatureEffect$new(predictor, feature = \"chloride\", method = \"pdp\")\ngg_pdp &lt;- plot(pdp) + xlab(\"Chloride (mmol/L)\") + scale_y_continuous(name = \"Average Marginal Impact\") + ggtitle(\"Partial Dependence Plot\") + coord_cartesian(xlim = c(80, 140))\n\n\n\n\n\nAccumulated Local Effects (ALE) plots are another global explainability technique that can help us understand the relationship between a feature and the model’s predictions. ALE plots show the average effect of changing a feature while accounting for the effects of other features.\n\n\nShow the Code\npredictor &lt;- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |&gt; select(any_of(predictors))), y = validation_with_predictions[[\".pred_1\"]], predict.function = predict_wrapper, type = \"prob\", class = 2)\n\nale &lt;- FeatureEffect$new(predictor, feature = \"chloride\")\ngg_ale &lt;- plot(ale) + scale_x_continuous(name = \"Chloride (mmol/L)\") + \n    scale_y_continuous(name = \"Average Conditional Impact\") + \n    scico::scale_fill_scico(palette = \"lipari\", begin = 0.1, end = 0.9, name = \"Impact on Prediction\") + \n    coord_cartesian(xlim = c(80, 140)) + \n    ggtitle(\"Accumulated Local Effects Plot\")\n\nggpubr::ggarrange(gg_pdp, gg_ale, ncol = 2, nrow = 1)\n\n\n\n\n\nWhile As we can see, we obtain slightly different answers from each approach, though largely they agree. Higher chloride concentrations lead to high predicted probabilities for contamination by normal saline. Some of the idiosyncrasies of each approach are summarized below.\n\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nPartial Dependence Plots (PDP)\n\nIntuitive interpretation.\nCausal relationship.\n\n\nAssumes independence between features.\nComputationally expensive.\n\n\n\nAccumulated Local Effects (ALE)\n\nCan handle correlated features.\nFast and cheap to calculate.\n\n\nBinning leads to odd results across intervals.\nNaive to heterogeneity across feature effects."
  },
  {
    "objectID": "explainability.html#local-explainability",
    "href": "explainability.html#local-explainability",
    "title": "Explaining Models and Predictions",
    "section": "",
    "text": "Local explainability refers to the ability to explore the effect of each feature on a model’s prediction for any given set of inputs. For our IV fluid detection example, let’s use the following set of BMP results.\n\n\nShow the Code\n# Pick a random highly positive example\nlocal_example &lt;- validation_with_predictions |&gt; arrange(desc(.pred_1)) |&gt; slice_head(n = 1) \n\n# Rename columns without _delta_prior\ndeltas &lt;- local_example |&gt; select(matches(\"_delta_prior\")) |&gt; rename_all(~str_remove(.x, \"_delta_prior\"))\n\n# Print a table of deltas and results\nexample_table &lt;- as.data.frame(bind_rows(local_example |&gt; select(any_of(predictors)) |&gt; select(-matches(\"_delta_prior\")), local_example |&gt; select(any_of(predictors)) |&gt; select(-matches(\"_delta_prior\")) + deltas))\nrow.names(example_table) &lt;- c(\"Prior\", \"Current\")\n\nknitr::kable(example_table, digits = 2, row.names = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsodium\nchloride\npotassium_plas\nco2_totl\nbun\ncreatinine\ncalcium\nglucose\n\n\n\n\nPrior\n142\n120\n2.8\n16\n12\n0.78\n6.0\n117\n\n\nCurrent\n145\n132\n1.3\n9\n8\n0.49\n3.7\n-23\n\n\n\n\n\n\n\nSHAP values4 have become a staple in tabular model interpretation.\n\n\nShow the Code\n# Load Libraries\nlibrary(shapviz)\n\n# Build SHAP explainer\nshap_local &lt;- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(local_example |&gt; select(any_of(predictors)) %&gt;% bake(recipe, .)), X = local_example)\nshap_local$S &lt;- as.matrix(shap_local$S * -1)\n\n# Plot SHAP Values Locally\nsv_waterfall(shap_local, show_annotation = F) + \n  ggtitle(\"Local Explanation of a Positive Prediction with SHAP\") + \n  scico::scale_fill_scico_d(palette = \"vik\", begin = 0.9, end = 0.1) +\n  theme(plot.title = element_text(size = 18, face = \"bold.italic\"))\n\n\n\n\n\nThe local SHAP values for this example show that the high probability prediction is driven largely by the increase in chloride, the high chloride result, and the decrease in calcium. This aligns with our a priori hypotheses as to what saline-contaminated results should look like, adding confidence in the prediction.\n\n\nWe can also aggregate local SHAP values to understand the global importance of each feature in the model’s predictions in the form of a “beeswarm” plot.\n\n\nShow the Code\n# Build SHAP explainer\nshap &lt;- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(validation_with_predictions |&gt; select(any_of(predictors)) %&gt;% bake(recipe, .)), X = as.data.frame(validation_with_predictions))\n\n\n[20:01:48] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\n\nShow the Code\nshap$S &lt;- as.matrix(shap$S * -1)\n\n# Plot SHAP Values as Beeswarm Plot\ngg_bee &lt;- sv_importance(shap, kind = \"beeswarm\", max_display = 5, alpha = 0.75) + scico::scale_color_scico(palette = \"vik\", breaks = c(0, 1), labels = c(\"Low\", \"High\"), name = \"Feature Value\") + xlab(\"Impact on Prediction\") + theme(legend.position = c(0.85, 0.20), legend.direction = \"horizontal\", legend.title.position = \"top\", axis.text.x.bottom = element_blank())\ngg_bee\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaway:\nML models, and their predictions, can be explained using tools such as SHAP values."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "As this site aims to serve as a practical guide to the technical details of these machine learning concepts, we have provided example code in each section that will allow you to take the publicly available data and models and reproduce the figures you see as you explore each section. Below, we will outline how to set your environment up so that you can code along with the material.\n\n\n\n\n\n\nThis code walk-through assumes R is already installed, and you have a basic familiarity with running R code.\n\n\n\n\n\n\nManual DownloadUsing Git\n\n\n\nDownload the repo:\nhttps://github.com/nspies13/practical-machine-learning-in-the-clinical-laboratory/archive/refs/heads/main.zip\nOpen the zip file and move the contents to your desired location on your computer.\n\n\n\nClone the repository by running this command from the desired location on your computer as your working directory.\ngit clone https://github.com/nspies13/practical-machine-learning-in-the-clinical-laboratory.git\n\n\n\nOnce downloaded, open the project environment by clicking on the file “supplementary_website.Rproj”.\n\n\n\nWith the .Rproj file opened, our first task will be to install the necessary packages by running the code below.\n\n### Install renv if you don't already have it\nif (!requireNamespace(\"renv\", quietly = TRUE)) {\n  install.packages(\"renv\")\n}\n\n### Use renv to install the necessary packages\nrenv::restore()\n\nNext, we’ll load the most relevant libraries and set a random seed for reproducibility.\n\n## Load Libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n## Configure Environment\nset.seed(12345)\n\n\n\n\n\n## Download and Import BMP Data Directly from FigShare \ntrain &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407401\")\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\")\n\n## Download and Import Models Directly from FigShare\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nmodel_retrospective &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631491\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\n\nWith these commands run, you should be all set to code along with the remainder of the content in this site."
  },
  {
    "objectID": "getting_started.html#the-github-repository",
    "href": "getting_started.html#the-github-repository",
    "title": "Getting Started",
    "section": "",
    "text": "Manual DownloadUsing Git\n\n\n\nDownload the repo:\nhttps://github.com/nspies13/practical-machine-learning-in-the-clinical-laboratory/archive/refs/heads/main.zip\nOpen the zip file and move the contents to your desired location on your computer.\n\n\n\nClone the repository by running this command from the desired location on your computer as your working directory.\ngit clone https://github.com/nspies13/practical-machine-learning-in-the-clinical-laboratory.git\n\n\n\nOnce downloaded, open the project environment by clicking on the file “supplementary_website.Rproj”."
  },
  {
    "objectID": "getting_started.html#setting-up-our-environment",
    "href": "getting_started.html#setting-up-our-environment",
    "title": "Getting Started",
    "section": "",
    "text": "With the .Rproj file opened, our first task will be to install the necessary packages by running the code below.\n\n### Install renv if you don't already have it\nif (!requireNamespace(\"renv\", quietly = TRUE)) {\n  install.packages(\"renv\")\n}\n\n### Use renv to install the necessary packages\nrenv::restore()\n\nNext, we’ll load the most relevant libraries and set a random seed for reproducibility.\n\n## Load Libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n## Configure Environment\nset.seed(12345)"
  },
  {
    "objectID": "getting_started.html#download-and-load-the-example-data-and-models",
    "href": "getting_started.html#download-and-load-the-example-data-and-models",
    "title": "Getting Started",
    "section": "",
    "text": "## Download and Import BMP Data Directly from FigShare \ntrain &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407401\")\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\")\n\n## Download and Import Models Directly from FigShare\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nmodel_retrospective &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631491\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\n\nWith these commands run, you should be all set to code along with the remainder of the content in this site."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Machine Learning in the Clinical Laboratory",
    "section": "",
    "text": "Welcome to Practical Machine Learning in the Clinical Laboratory. This site aims to serve as a supplement to the content outlined in the article, “Validating, Implementing, and Monitoring Machine Learning Solutions in the Clinical Laboratory Safely and Effectively”. We hope to provide a more detailed, technical corollary to the concepts and principles discussed in the main article.\nThe site will guide us through some of the practical components of applying machine learning to clinical laboratory tasks using a real-world example, the detection of basic metabolic panel (BMP) results that have been contaminated by 0.9% normal saline (NS). The data and models used in this example are publicly available on FigShare (see Getting Started), and the code will be written in R."
  },
  {
    "objectID": "index.html#the-motivating-example",
    "href": "index.html#the-motivating-example",
    "title": "Practical Machine Learning in the Clinical Laboratory",
    "section": "The Motivating Example",
    "text": "The Motivating Example\nErroneous laboratory results contribute to a cascade of downstream consequences that negatively impact patient care1, including delays in diagnosis, incorrect treatments, and increased healthcare costs2–4. The majority of these errors stem from issues of improper collection or transport, and occur prior to a specimen reaching the laboratory5,6. While substantial progress has been made in reducing the burden of mislabeled specimens, improperly ordered tests, and other preanalytical errors7–11, contamination by IV fluids remains an unsolved problem12,13. Recognizing this unmet need, the IFCC Working Group on Laboratory Error and Patient Safety added a new quality indicator – “Contamination by a non-microbiological source (Pre-Cont)” to its 2019 report14.\n\n\n\nFigure 1: Preanalytical errors are common, and occur prior to specimens reaching the laboratory."
  },
  {
    "objectID": "index.html#iv-fluid-contamination",
    "href": "index.html#iv-fluid-contamination",
    "title": "Practical Machine Learning in the Clinical Laboratory",
    "section": "IV Fluid Contamination",
    "text": "IV Fluid Contamination\nIV fluid contamination occurs when a sample is collected from a catheter through which a solution is being infused or drawn proximally to the catheter’s insertion site. This leads to divergence in the measured concentrations for all analytes being tested, the nature of which depend on the composition of the contaminating fluid (Figure 2). Current protocols for detecting contaminated specimens vary across institutions, and may rely on delta checks, feasibility flags, or manual technologist review. These methods are often time-consuming and may prone to error15. The multivariate nature of this problem lends itself well to a machine learning solution.\n\n\n\nFigure 2: Preanalytical errors, such as IV fluid contamination, often display an “anomaly-with-resolution” pattern across multiple analytes if recollected with proper technique."
  },
  {
    "objectID": "index.html#the-machine-learning-solution",
    "href": "index.html#the-machine-learning-solution",
    "title": "Practical Machine Learning in the Clinical Laboratory",
    "section": "The Machine Learning Solution",
    "text": "The Machine Learning Solution\n~2,500,000 BMP results collected from inpatients at a single institution were extracted from the laboratory information system. Contamination by 0.9% normal saline was simulated16 at varying mixture ratios in a randomly selected subset of results (Figure 3). An XGBoost17 model was tuned using cross-validation, then trained to predict the binary class label of simulated contamination vs. physiologic result.\nTwo models will be described in this example:\n\nA real-time model that uses the patients’ current and most recent prior results to predicts contamination at the time the specimen is drawn.\nA retrospective model which also incorporates patients’ subsequent results to assess for the anomaly-with-resolution pattern.\n\nThe real-time model would be intended for live clinical use, while the retrospective model would be intended as a quality assurance tool and mechanism by which ground truth labels could be applied in an automated, scalable fashion.\n\n\n\nFigure 3: Simulating normal saline contamination to generate labels for training the XGBoost model."
  },
  {
    "objectID": "applicability.html",
    "href": "applicability.html",
    "title": "Assessing Model Applicability",
    "section": "",
    "text": "An effective ML pipeline must be able to recognize when predictions should and should NOT be made. Applicability domain assessment, or applicability, refers to the identification of inputs that are too different from the training data for the pipeline to render a reliable prediction. These are termed, “out-of-distribution” inputs, may be due to measurement error, labeling errors, or other pre-analytical factors.\nIn their simplest form, these can be a series of univariate decision boundaries that define the range of values for each feature that are considered safe for prediction, with the rest labeled “out-of-distribution”. Feasibility limits or linearity limits make good examples of these within the clinical laboratory. However, multivariate approaches are more common in practice, as they can account for the interactions between features.\n\n\n\n\nThe Mahalanobis distance is a measure of the distance between a point and a distribution. It is a multivariate generalization of the idea of measuring how many standard deviations away a point is from the mean of a distribution. We can use the Mahalanobis distance to identify out-of-distribution inputs in R using the code below.\n\n\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n# Load Model and Data\n## Load models\noptions(timeout=300)\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nrecipe &lt;- model_realtime |&gt; extract_recipe()\n\n## Load data\ntrain &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407401\")\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\")\n\n## Preprocess Data\ntrain_preprocessed &lt;- bake(recipe, train)\nvalidation_preprocessed &lt;- bake(recipe, validation)\n\n## Calculate Mahalanobis Distance from Training Set for a Subset of Points in Validation Set\nmahalanobis_distance &lt;- function(data, train_preprocessed) {\n  \n  train_mean &lt;- colMeans(train_preprocessed, na.rm = T)\n  train_cov &lt;- cov(train_preprocessed, use = \"pairwise.complete.obs\")\n  mahalanobis(data, train_mean, train_cov, inverted = TRUE)\n  \n}\n\n## Calculate Mahalanobis Distance for Validation Set\ntrain_distances &lt;- mahalanobis_distance(train_preprocessed, train_preprocessed)\nvalidation_distances &lt;- mahalanobis_distance(validation_preprocessed, train_preprocessed)\nupper_bound &lt;- quantile(train_distances, probs = c(0.999), na.rm = T)\n\n## Plot Distances\ngg_maha_dist_input &lt;- bind_rows(tibble(label = \"Train\", distance = train_distances), tibble(label = \"Validation\", distance = validation_distances)) \n\ngg_maha_dist &lt;- \n  ggplot(gg_maha_dist_input |&gt; dplyr::filter(label == \"Train\") |&gt; slice_sample(prop = 0.01), aes(x = distance)) +\n  stat_ecdf() +\n  geom_vline(xintercept = upper_bound, linetype = \"dashed\") +\n  annotate(\"text\", x = upper_bound, y = 0.5, label = \"Out-of-Distribution\", angle = -90, hjust = 0, vjust = -0.5, fontface = \"bold\") +\n  labs(title = \"Mahalanobis Distance from Training Set\",\n       x = \"Mahalanobis Distance\",\n       y = \"Cumulative Proportion\") + \n  scale_x_log10()\n\nsuppressWarnings(gg_maha_dist)\n\n\n\n\n\n\n\n\nShow the Code\nlibrary(gt)\n\nvalidation_preprocessed_with_distances &lt;- validation_preprocessed |&gt; mutate(mahalanobis_distance = validation_distances)\n\nvalidation |&gt; select(sodium, chloride, potassium_plas, co2_totl, bun, creatinine, calcium, glucose) |&gt; bind_cols(validation_preprocessed_with_distances |&gt; select(matches(\"dist\"))) |&gt; arrange(desc(mahalanobis_distance)) |&gt; slice_head(n = 1) |&gt; select(-matches(\"delta|dist|_id|dt_tm\")) |&gt; pivot_longer(cols = everything(), names_to = \"Analyte\", values_to = \"Result\") |&gt; gt() |&gt; tab_header(\"Example Out-of-Distribution BMP\")\n\n\n\n\n\n\n  \n    \n      Example Out-of-Distribution BMP\n    \n    \n    \n      Analyte\n      Result\n    \n  \n  \n    sodium\n83.00\n    chloride\n60.00\n    potassium_plas\n1.70\n    co2_totl\n6.00\n    bun\n2.00\n    creatinine\n0.28\n    calcium\n5.00\n    glucose\n1500.00\n  \n  \n  \n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis (PCA) is technique that can be used to collapse correlated features into a simpler representation. With the help of the {applicable}1 package, we can use PCA to identify out-of-distribution inputs in R using the code below.\n\n\nShow the Code\n## Load package\nlibrary(applicable)\n\n## Train PCA Model\ntrain_pca &lt;- apd_pca(train_preprocessed |&gt; drop_na())\n\n## Calculate Distance in PC space\npca_score &lt;- score(train_pca, validation_preprocessed_with_distances)\n\n## Add Distance to Validation Data\nvalidation_preprocessed_with_distances &lt;- validation_preprocessed_with_distances |&gt; mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = ntile(mahalanobis_distance, n = 1000))\n\n## Plot PCA Distance as compared to Mahalanobis Distance\ngg_pca_dist &lt;-\n  ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_pctl)) +\n    geom_point(alpha = 0.1, shape = \".\") + \n    scale_x_continuous(name = \"PCA Distance Percentile\", breaks = c(0, 50, 100), labels = c(0, 50, 100)) + \n    scale_y_continuous(name = \"Mahalanobis Percentile\", breaks = c(0, 500, 1000), labels = c(0, 50, 100)) + \n    ggtitle(\"Comparison of Distance Metrics for Multivariate Applicability Assessment\")\n\ngg_pca_dist\n\n\n\n\n\nWhile the PCA distance and the Mahalanobis distance are correlated, there are outliers along each axis that warrant further investigation. In general, PCA distance is more appropriate as input features become higher in dimensionality, or more correlated.\n\n\n\n\n\n\nKey Takeaway:\nPredictions made on out-of-distribution inputs should be interpreted with extreme caution.  Applicability assessment provides tools for identifying these cases."
  },
  {
    "objectID": "applicability.html#multivariate-methods",
    "href": "applicability.html#multivariate-methods",
    "title": "Assessing Model Applicability",
    "section": "",
    "text": "The Mahalanobis distance is a measure of the distance between a point and a distribution. It is a multivariate generalization of the idea of measuring how many standard deviations away a point is from the mean of a distribution. We can use the Mahalanobis distance to identify out-of-distribution inputs in R using the code below.\n\n\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n# Load Model and Data\n## Load models\noptions(timeout=300)\nmodel_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nrecipe &lt;- model_realtime |&gt; extract_recipe()\n\n## Load data\ntrain &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407401\")\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\")\n\n## Preprocess Data\ntrain_preprocessed &lt;- bake(recipe, train)\nvalidation_preprocessed &lt;- bake(recipe, validation)\n\n## Calculate Mahalanobis Distance from Training Set for a Subset of Points in Validation Set\nmahalanobis_distance &lt;- function(data, train_preprocessed) {\n  \n  train_mean &lt;- colMeans(train_preprocessed, na.rm = T)\n  train_cov &lt;- cov(train_preprocessed, use = \"pairwise.complete.obs\")\n  mahalanobis(data, train_mean, train_cov, inverted = TRUE)\n  \n}\n\n## Calculate Mahalanobis Distance for Validation Set\ntrain_distances &lt;- mahalanobis_distance(train_preprocessed, train_preprocessed)\nvalidation_distances &lt;- mahalanobis_distance(validation_preprocessed, train_preprocessed)\nupper_bound &lt;- quantile(train_distances, probs = c(0.999), na.rm = T)\n\n## Plot Distances\ngg_maha_dist_input &lt;- bind_rows(tibble(label = \"Train\", distance = train_distances), tibble(label = \"Validation\", distance = validation_distances)) \n\ngg_maha_dist &lt;- \n  ggplot(gg_maha_dist_input |&gt; dplyr::filter(label == \"Train\") |&gt; slice_sample(prop = 0.01), aes(x = distance)) +\n  stat_ecdf() +\n  geom_vline(xintercept = upper_bound, linetype = \"dashed\") +\n  annotate(\"text\", x = upper_bound, y = 0.5, label = \"Out-of-Distribution\", angle = -90, hjust = 0, vjust = -0.5, fontface = \"bold\") +\n  labs(title = \"Mahalanobis Distance from Training Set\",\n       x = \"Mahalanobis Distance\",\n       y = \"Cumulative Proportion\") + \n  scale_x_log10()\n\nsuppressWarnings(gg_maha_dist)\n\n\n\n\n\n\n\n\nShow the Code\nlibrary(gt)\n\nvalidation_preprocessed_with_distances &lt;- validation_preprocessed |&gt; mutate(mahalanobis_distance = validation_distances)\n\nvalidation |&gt; select(sodium, chloride, potassium_plas, co2_totl, bun, creatinine, calcium, glucose) |&gt; bind_cols(validation_preprocessed_with_distances |&gt; select(matches(\"dist\"))) |&gt; arrange(desc(mahalanobis_distance)) |&gt; slice_head(n = 1) |&gt; select(-matches(\"delta|dist|_id|dt_tm\")) |&gt; pivot_longer(cols = everything(), names_to = \"Analyte\", values_to = \"Result\") |&gt; gt() |&gt; tab_header(\"Example Out-of-Distribution BMP\")\n\n\n\n\n\n\n  \n    \n      Example Out-of-Distribution BMP\n    \n    \n    \n      Analyte\n      Result\n    \n  \n  \n    sodium\n83.00\n    chloride\n60.00\n    potassium_plas\n1.70\n    co2_totl\n6.00\n    bun\n2.00\n    creatinine\n0.28\n    calcium\n5.00\n    glucose\n1500.00\n  \n  \n  \n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis (PCA) is technique that can be used to collapse correlated features into a simpler representation. With the help of the {applicable}1 package, we can use PCA to identify out-of-distribution inputs in R using the code below.\n\n\nShow the Code\n## Load package\nlibrary(applicable)\n\n## Train PCA Model\ntrain_pca &lt;- apd_pca(train_preprocessed |&gt; drop_na())\n\n## Calculate Distance in PC space\npca_score &lt;- score(train_pca, validation_preprocessed_with_distances)\n\n## Add Distance to Validation Data\nvalidation_preprocessed_with_distances &lt;- validation_preprocessed_with_distances |&gt; mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = ntile(mahalanobis_distance, n = 1000))\n\n## Plot PCA Distance as compared to Mahalanobis Distance\ngg_pca_dist &lt;-\n  ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_pctl)) +\n    geom_point(alpha = 0.1, shape = \".\") + \n    scale_x_continuous(name = \"PCA Distance Percentile\", breaks = c(0, 50, 100), labels = c(0, 50, 100)) + \n    scale_y_continuous(name = \"Mahalanobis Percentile\", breaks = c(0, 500, 1000), labels = c(0, 50, 100)) + \n    ggtitle(\"Comparison of Distance Metrics for Multivariate Applicability Assessment\")\n\ngg_pca_dist\n\n\n\n\n\nWhile the PCA distance and the Mahalanobis distance are correlated, there are outliers along each axis that warrant further investigation. In general, PCA distance is more appropriate as input features become higher in dimensionality, or more correlated.\n\n\n\n\n\n\nKey Takeaway:\nPredictions made on out-of-distribution inputs should be interpreted with extreme caution.  Applicability assessment provides tools for identifying these cases."
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary of Key Terms",
    "section": "",
    "text": "Glossary of Key Terms\n\n\n\nArtificial Intelligence (AI)\nA computer system that can simulate or perform human tasks.\n\n\nMachine Learning (ML)\nA discipline within artificial intelligence (AI) dedicated to the development of algorithms that improve their performance on a human task without requiring explicit instructions.\n\n\nML Algorithm/Model\nA learned representation of the patterns inherent with the input data that can be used to generate predictions.\n\n\nPipeline\nA full sequence of steps to convert input data into output predictions. Typically involving loading, reformatting, and transforming data and predictions so that they can be integrating into real-time workflows.\n\n\nDevelopment Operations (DevOps)\nA philosophical framework combining best practices in information technology operations and software engineering to rapidly and robustly build and implement high-quality informatics solutions.\n\n\nMachine Learning Operations (MLOps)\nA framework of technical best practices for deploying and maintaining machine learning applications efficiently and effectively.\n\n\nTarget/Ground-Truth\nThe “gold-standard” definition to which ML pipeline predictions will be compared.\n\n\nDiscriminative Performance\nThe degree to which predictions match the ground-truth labels, often measured by sensitivity, specificity, positive predictive value, and negative predictive value.\n\n\nImplementation Efficacy\nThe degree to which the final implementation of the ML pipeline satisfies the original need for which it was built.\n\n\nReceiver Operating Characteristic (ROC Curve)\nA visualization of the trade-off between sensitivity and specificity across the full breadth of possible decision thresholds for a continuous output.\n\n\nClass Imbalance\nThe degree to which the proportion of class labels are skewed towards one label or the other.\n\n\nPrecision and Recall Curve\nA visualization of the trade-off between precision (positive predictive value) and recall (sensitivity) across the full breadth of possible decision thresholds for a continuous output.\n\n\nF1 Score\nThe harmonic mean of precision and recall. See more here.\n\n\nMatthews Correlation Coefficient\nThe Pearson correlation coefficient for two binary variables. See more here.\n\n\nCost-Sensitive Learning\nAn learning approach where each classification error is assigned its own weight to fine-tune the model’s predilection towards certain error type1 2.\n\n\nEquivocal Zone\nAn interval of continuous output in which no binary class label is assigned.\n\n\nApplicability Assessment\nThe determination of whether a new input is similar enough to a model’s training data for a reliable prediction.\n\n\nDemographic Parity\nA fairness criterion used to assess whether the outputs of a predictive model is independent of demographic groups (e.g. race, gender, or age).\n\n\nPredictive Parity\nA fairness criterion used to assess whether the outputs of a model have equal positive and negative predictive values across demographic groups (e.g. race, gender, or age).\n\n\nEqualized Odds\nA fairness criterion used to assess whether the outputs of a model have equal true positive rates (sensitivity) and false positive rates (specificity) across demographic groups (e.g. race, gender, or age).\n\n\nGlobal Explainability\nThe ability to estimate each feature’s impact on model outputs aggregated across an entire data set or feature space.\n\n\nLocal Explainability\nThe ability to estimate each feature’s impact on model outputs for any given individual prediction.\n\n\nGovernance\nThe processes by which organizational responsibilities and decisions are divided, evaluated, and executed.\n\n\nDeployment\nMaking a model or application accessible to other computers within a network.\n\n\nProduction Environment\nThe software systems and infrastructure in which live applications are hosted and run for day-to-day operations.\n\n\nDevelopment Environment\nAn isolated copy of the production environment where software changes can be tested without risk of impacting live operations.\n\n\nApplication-Programming Interface (API)\nA protocol or framework by which various software applications can communicate with each other and exchange data or predictions.\n\n\nHuman-in-the-Loop\nAn implementation paradigm where model outputs are directed towards an expert user for incorporating into their decision-making before an action is taken.\n\n\nData Drift\nDivergence in input data away from the initial model training data set.\n\n\nConcept Drift\nDivergence away from the training data in the target labels or context in which predictions are to be made.\n\n\nContinuous Integration (CI)\nA DevOps principle in which changes to software are incorporate in small, manageable chunks continuously rather than large overhauls.\n\n\nContinuous Deployment (CD)\nA DevOps principle in which updates to software a pushed to live environments without large periods of maintenance or down-time.\n\n\n\n\n\n\n\n\nReferences\n\n1. Ling CX, Sheng VS. Cost-Sensitive Learning [Internet]. In: Sammut C, Webb GI, editors. Boston, MA: Springer US; 2011. p. 231–5.Available from: https://link.springer.com/10.1007/978-0-387-30164-8_181\n\n\n2. Mienye ID, Sun Y. Performance analysis of cost-sensitive learning methods with application to imbalanced medical data. Informatics in Medicine Unlocked [Internet] 2021;25:100690. Available from: https://linkinghub.elsevier.com/retrieve/pii/S235291482100174X"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nShow the Code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "ground_truth.html",
    "href": "ground_truth.html",
    "title": "Establishing Ground Truth Labels",
    "section": "",
    "text": "Establishing Ground Truth Labels\n\nAssessing the performance of a machine learning pipeline in the clinical laboratory requires that we compare its predictions to some form of ground truth label. Given its importance, it is crucial that we perform a comprehensive evaluation of the options for defining ground truth. In clinical applications, predictions often must be made at time points with incomplete information. However, our ground truth label can often benefit from its retrospective or asynchronous nature by incorporating valuable information that is not available to models in real-time. Below, we explore the differences between various options for ground truth labels in our IV fluid contamination example.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGround Truth\nExample\n\n\n\n\nCurrent State\nTechnologist-applied interpretive comments\n\n\nReal-Time Deltas\nMultivariate delta checks from mixing experiments1\n\n\nPre-Post Deltas\nData-derived thresholds for anomaly-resolution2\n\n\nRetrospective ML\nML models that use the subsequent draw as features\n\n\nExpert Review\nSubject matter experts adjudicate each result\n\n\n\n\n\nThe code chunk below demonstrates how we can define each of these options for ground truth labeling of our validation set in R, so that we can assess their similarities and differences. Comparing each option can be achieved on the aggregate using a Venn diagram, but for a more granular exploration of the data, we can use a tile plot instead, shown below.\n\n\nShow the Code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\n\n## Load models\n#model_realtime &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631488\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\nmodel_retrospective &lt;- read_rds(\"https://figshare.com/ndownloader/files/45631491\") |&gt; pluck(\"model\") |&gt; bundle::unbundle()\npredictors &lt;- model_retrospective |&gt; extract_recipe() |&gt; pluck(\"term_info\") |&gt; dplyr::filter(role == \"predictor\") |&gt; pluck(\"variable\")\n\n## Load validation data\nvalidation &lt;- arrow::read_feather(\"https://figshare.com/ndownloader/files/45407398\") |&gt; select(any_of(predictors), contam_comment, expert_review_prediction)\n\n## Add ground truth labels\nvalidation_with_ground_truth_labels &lt;- \n  validation %&gt;% \n    transmute(\n      ## Current State Labels: Technologist- and Delta Check-Driven Interpretive Comments\n      current_state = contam_comment,\n      \n      ## Literature-based Multivariate Delta Checks from Choucair et al. 2022\n      realtime_deltas = chloride_delta_prior &gt; 7.7 & potassium_plas_delta_prior &lt; -0.7 & calcium_delta_prior &lt; -1.7,\n      \n      ## Retrospective Rules: Data-derived thresholds for resolution back to baseline from Patel et al. 2015\n      retrospective_deltas = \n        chloride_delta_prior &gt; quantile(chloride_delta_prior, probs = 0.95, na.rm = T)[[1]] & \n        chloride_delta_post &lt; quantile(chloride_delta_post, probs = 0.05, na.rm = T)[[1]] &\n        calcium_delta_prior &lt; quantile(calcium_delta_prior, probs = 0.05, na.rm = T)[[1]] &\n        calcium_delta_post &gt; quantile(calcium_delta_post, probs = 0.95, na.rm = T)[[1]],\n      \n      ## Retrospective ML: ML models that use the subsequent draw as features\n      retrospective_ml = (predict(model_retrospective, new_data = .) |&gt; pluck(\".pred_class\") == 1),\n      \n      ## Expert Review: Subject matter experts adjudicate each result\n      expert_review = expert_review_prediction == 1 & glucose_delta_prior &lt; 100 & calcium_delta_prior &lt; 0\n    )\n\n## Convert to Long Format for Visualization with ggplot2\nground_truth_tile_plot_input &lt;- \n  validation_with_ground_truth_labels |&gt;\n    slice_sample(n = 100, by = expert_review) |&gt; ## Select 1000 Random Negatives and Positives from ML\n    mutate(id = row_number()) |&gt; ## Add an index column for plotting\n    pivot_longer(-id, names_to = \"Type\", values_to = \"Value\") |&gt;\n    mutate(Value = factor(Value, labels = c(\"Negative\", \"Positive\")), \n           Type = factor(Type, \n                         levels = c(\"current_state\", \"realtime_deltas\", \"retrospective_deltas\", \"retrospective_ml\", \"expert_review\"),\n                         labels = c(\"Current State\", \"Real-Time Deltas\", \"Retrospective Deltas\", \"Retrospective ML\", \"Expert Review\")))\n\n## Plot the Tile Plot\ngg_tile &lt;- \n  ggplot(ground_truth_tile_plot_input, aes(x = id, y = fct_rev(Type), fill = Value)) +\n    geom_tile(alpha = 0.75) +\n    geom_vline(xintercept = c(100.5, 200.5), linetype = \"dashed\") +  \n    geom_text(data = tibble(id = c(50, 150, 250), Type = c(\"Expert Review\", \"Expert Review\", \"Expert Review\"), Value = c(NA, \"Negative\", \"Positive\"), Label = c(\"Not Available\", \"Negative\", \"Positive\")), aes(label = Label, color = Value), fontface = \"bold.italic\", size = 8) +\n    scale_x_continuous(name = \"Specimen ID\", expand = c(0,0)) + \n    scale_y_discrete(name = \"Ground Truth Type\", expand = c(0,0)) +\n    scale_fill_manual(values = c(scico::scico(2, palette = \"lipari\", begin = 0.6333, end = 0.3666)), na.value = scico::scico(1, palette = \"lipari\", begin = 0.9, end = 0.9)) +\n    scale_color_manual(values = c(scico::scico(2, palette = \"lipari\", begin = 0.6, end = 0.35)), na.value = scico::scico(1, palette = \"lipari\", begin = 0.85, end = 0.85)) +\n    guides(alpha = \"none\") +\n    ggtitle(\"Ground Truth Label Comparison\", subtitle = \"100 Randomly-Selected Specimens From Each Expert Review Label\") +\n    theme(legend.position = \"none\", legend.title = element_blank(), legend.direction = \"horizontal\", axis.text.x.bottom = element_blank(), axis.title.y.left = element_blank())\n\ngg_tile\n\n\n\n\n\nHere, we can see remarkable variation across 5 different options for assigning our ground truth for cases that are not labeled as negative by the expert reviewers. Our current state labels seem to be quite insensitive as compared to the other options, but it does benefit from being available in all cases. Meanwhile, both multivariate delta check methods will catch more cases than our current state. The most similar, however, is the retrospective ML labels, likely due to an ML-based approach’s potential for capturing complex, non-linear relationships between each feature.\nWhile having a subject matter expert review all predictions retrospectively may be an ideal solution, it often requires an unfeasible amount of investment. Additionally, it should not be taken as an immutable truth that the human-defined expert labels are in fact better than what an ML model can do, as many problems are extraordinarily complex, and require balancing many dimensions simultaneously – a task for which humans are ill-suited.\nThe high negative and positive percent agreements, combined with operational advantages of an automatable approach, make the retrospective ML-based gold-standard definition a particularly attractive one for the problem of IV fluid contamination detection, and we will proceed with those labels as our ground truth for the rest of this exercise.\n\n\n\n\n\n\nKey Takeaway:\nThe choice of ground truth label should strive to achieve an optimal balance of performance and feasibility.\n\n\n\n\n\n\n\n\nReferences\n\n1. Choucair I, Lee ES, Vera MA, Drongmebaro C, El-Khoury JM, Durant TJS. Contamination of clinical blood samples with crystalloid solutions: An experimental approach to derive multianalyte delta checks. Clinica Chimica Acta [Internet] 2023;538:22–8. Available from: https://linkinghub.elsevier.com/retrieve/pii/S0009898122013456\n\n\n2. Patel DK, Naik RD, Boyer RB, Wikswo J, Vasilevskis EE. Methods to identify saline-contaminated electrolyte profiles. Clinical Chemistry and Laboratory Medicine 2015;53(10):1585–91."
  },
  {
    "objectID": "activity2_wargames.html",
    "href": "activity2_wargames.html",
    "title": "Activity 2: Institutional Complexities in AI Deployment",
    "section": "",
    "text": "Activity 2: Institutional Complexities in AI Deployment\n\n\n\nIntroduction\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse volutpat ex sed erat pellentesque, bibendum ullamcorper velit tincidunt. Sed at nisl in nisi auctor ullamcorper vel a urna. Cras eget enim a est dignissim tempus eu non lacus. Donec dignissim purus lacus, id euismod ligula placerat sed. Duis placerat risus risus. Pellentesque aliquet, enim feugiat facilisis congue, elit nisi scelerisque ligula, eu elementum ligula ipsum sed odio. Sed ornare convallis nisi nec lobortis. Sed molestie tincidunt nunc eget elementum. Vivamus posuere dolor id elit molestie, quis convallis nisi aliquet. Vestibulum nunc tellus, ullamcorper et accumsan vitae, placerat quis libero. Morbi pulvinar, tortor et finibus pretium, erat elit rutrum nisi, eget iaculis ligula massa nec lorem.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus consectetur odio eu arcu dictum iaculis. Aliquam faucibus scelerisque magna, a tincidunt ex tincidunt quis. Vivamus porta id nisl ac hendrerit. Curabitur at urna pharetra augue feugiat pulvinar nec non turpis. Maecenas elementum erat leo, sed dignissim risus efficitur eget. Praesent elementum enim et risus molestie pharetra. Mauris urna massa, imperdiet vel placerat quis, finibus in leo. Vivamus id quam egestas, imperdiet elit ut, congue sem. Aliquam non tristique felis. Fusce arcu risus, bibendum eget nisi eget, tempor convallis mauris. Aliquam tincidunt eros sem, id imperdiet sapien sodales vel.\n\nIT/EngineeringLab (Medical Directorship)Compliance/RegulatoryData Science/AnalyticsFront-line Providers/Patients\n\n\n\n\n\n\n\n\nPrimary Responsibility\n\n\n\nEnsure secure, reliable, and scalable deployment, integration, and ongoing maintenance of AI systems within the existing institutional IT infrastructure.\n\n\n\n\n\n\n\n\nSecondary Responsibilities\n\n\n\n\nMaintain cybersecurity and data privacy.\nProvide technical support and incident response.\nOversee interoperability with laboratory and hospital information systems.\n\n\n\n\n\n\n\n\n\nDeal Breakers\n\n\n\n\nSecurity vulnerabilities or non-compliance with institutional IT policies.\nLack of interoperability or inability to integrate with existing systems.\nExcessive resource requirements or unsustainable maintenance burden.\n\n\n\n\n\n\n\n\n\n\n\nPrimary Responsibility\n\n\n\nEnsure that AI applications improve laboratory quality, efficiency, and patient outcomes without compromising clinical standards.\n\n\n\n\n\n\n\n\nSecondary Responsibilities\n\n\n\n\nServe as liaison between laboratory staff, clinicians, and technical teams.\nProvide clinical input on algorithm development and use case prioritization.\n\n\n\n\n\n\n\n\n\nDeal Breakers\n\n\n\n\nInsufficient validation or failure to meet clinical performance benchmarks.\nDisruption to laboratory operations or patient care.\nLack of clinician trust or engagement.\n\n\n\n\n\n\n\n\n\n\n\nPrimary Responsibility\n\n\n\nEnsure full compliance with all applicable regulatory, legal, and accreditation requirements (e.g., CLIA, CAP, HIPAA, FDA).\n\n\n\n\n\n\n\n\nSecondary Responsibilities\n\n\n\n\nGuide risk assessments and documentation practices.\nMonitor for ongoing regulatory changes impacting AI use.\nOversee management of adverse events and reporting obligations.\n\n\n\n\n\n\n\n\n\nDeal Breakers\n\n\n\n\nNon-compliance with regulatory or accreditation standards.\nInsufficient documentation or inability to audit system performance and decision-making.\n\n\n\n\n\n\n\n\n\n\n\nPrimary Responsibility\n\n\n\nDevelop, validate, and monitor AI models to ensure accuracy, fairness, and relevance to clinical needs.\n\n\n\n\n\n\n\n\nSecondary Responsibilities\n\n\n\n\nCollaborate with clinicians to define use cases and performance metrics.\nOversee validation, verification, and ongoing clinical performance monitoring of AI systems.\nProvide transparency regarding model limitations and potential biases.\n\n\n\n\n\n\n\n\n\nDeal Breakers\n\n\n\n\nInsufficient, biased, or unrepresentative data.\nLack of stakeholder engagement or infrastructure support.\n\n\n\n\n\n\n\n\n\n\n\nPrimary Responsibility\n\n\n\nEnsure that AI tools support and enhance patient care and improve provider experience.\n\n\n\n\n\n\n\n\nSecondary Responsibilities\n\n\n\n\nParticipate in user feedback and acceptability assessments.\nIdentify unintended consequences or workflow disruptions.\nProvide input on clinical relevance and user experience.\n\n\n\n\n\n\n\n\n\nDeal Breakers\n\n\n\n\nLoss of provider autonomy or increased workload.\nLack of trust or understanding regarding AI system recommendations."
  }
]