validation_fairness <-
validation_realtime_preds |>
fairness_metrics(truth = truth, estimate = pred, event_level = "second") |>
transmute(Metric = str_to_upper(.metric), `Metric Difference` = .estimate)
## Make fairness table
fairness_table <- validation_fairness |> knitr::kable(digits = 3)
## Calculate group-wise performance metrics
metrics_to_calculate <- metric_set(sens, spec, ppv, npv, detection_prevalence)
groupwise_metrics <-
validation_realtime_preds |>
group_by(sex) |>
metrics_to_calculate(truth = truth, estimate = pred, event_level = "second") |>
transmute(Metric = str_to_upper(.metric), Value = .estimate, Sex = sex)
groupwise_metrics <-
validation_realtime_preds |>
group_by(sex) |>
metrics_to_calculate(truth = truth, estimate = pred, event_level = "second") |>
transmute(Metric = str_to_upper(.metric), Value = .estimate, Sex = sex) |>
mutate(Metric = ifelse(Metric == "DETECTION_PREVALENCE", "FLAG RATE"))
groupwise_metrics <-
validation_realtime_preds |>
group_by(sex) |>
metrics_to_calculate(truth = truth, estimate = pred, event_level = "second") |>
transmute(Metric = str_to_upper(.metric), Value = .estimate, Sex = sex) |>
mutate(Metric = ifelse(Metric == "DETECTION_PREVALENCE", "FLAG RATE", Metric))
## Bind them together
all_metrics <- bind_rows(overall_metrics, groupwise_metrics)
## Output metrics comparison
all_table <- all_metrics |> pivot_wider(id_cols = Sex, names_from = Metric, values_from = Value) |> knitr::kable(digits = 3)
## Add predictive parity to yardstick's prebuilt functions
diff_range <- function(x) {diff(range(x$.estimate))}
predictive_parity <- new_groupwise_metric(ppv, name = "predictive_parity", aggregate = diff_range)
## Define fairness metrics to calculate
fairness_metrics <- metric_set(demographic_parity(by = sex), equalized_odds(by = sex), predictive_parity(by = sex))
## Calculate fairness metrics
validation_fairness <-
validation_realtime_preds |>
fairness_metrics(truth = truth, estimate = pred, event_level = "second") |>
transmute(Metric = str_to_upper(.metric), `Metric Difference` = .estimate)
## Make fairness table
fairness_table <- validation_fairness |> knitr::kable(digits = 3)
## Output metrics comparison
metric_table <- groupwise_metrics |> pivot_wider(id_cols = Sex, names_from = Metric, values_from = Value) |> knitr::kable(digits = 3)
#| echo: false
#|
## Print out the contingency table
table(validation_with_sex_labels$truth, validation_with_sex_labels$sex) |> gt::as_gt() |> tab_header(title = "Contaminated Results By Sex", subtitle = "Simulated labels, 2x higher prevalence in males")
#| echo: false
#|
## Print out the contingency table
table(validation_with_sex_labels$truth, validation_with_sex_labels$sex) |> gt::gt() |> gt::tab_header(title = "Contaminated Results By Sex", subtitle = "Simulated labels, 2x higher prevalence in males")
#| echo: false
#|
## Print out the contingency table
table(validation_with_sex_labels$truth, validation_with_sex_labels$sex) |> knitr::kable() |> gt::tab_header(title = "Contaminated Results By Sex", subtitle = "Simulated labels, 2x higher prevalence in males")
#| echo: false
#|
## Print out the contingency table
table(validation_with_sex_labels$truth, validation_with_sex_labels$sex) |> knitr::kable() |> gt() |> gt::tab_header(title = "Contaminated Results By Sex", subtitle = "Simulated labels, 2x higher prevalence in males")
#| echo: false
#|
## Print out the contingency table
table(validation_with_sex_labels$truth, validation_with_sex_labels$sex) |> knitr::kable() |> gt::gt() |> gt::tab_header(title = "Contaminated Results By Sex", subtitle = "Simulated labels, 2x higher prevalence in males")
#| echo: false
#|
## Print out the contingency table
table(validation_with_sex_labels$truth, validation_with_sex_labels$sex) |> knitr::kable(digits = 3, caption = "Contaminated Results by Simulated Sex Labels")
gg_pca_dist
validation_with_predictions
validation_with_predictions
#| fig-width: 12
#| fig-height: 5
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
# Load Model and Data
## Load models
options(timeout=300)
model_realtime <- read_rds("https://figshare.com/ndownloader/files/45631488") |> pluck("model") |> bundle::unbundle()
recipe <- model_realtime |> extract_recipe()
## Load data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401")
#train <- arrow::read_feather("../../data/anonymized_bmp_data_training_small.feather")
validation <- arrow::read_feather("https://figshare.com/ndownloader/files/45407398")
#validation <- arrow::read_feather("../../data/anonymized_bmp_data_validation_small.feather")
## Preprocess Data
train_preprocessed <- bake(recipe, train)
validation_preprocessed <- bake(recipe, validation)
## Calculate Mahalanobis Distance from Training Set for a Subset of Points in Validation Set
mahalanobis_distance <- function(data, train_preprocessed) {
train_mean <- colMeans(train_preprocessed, na.rm = T)
train_cov <- cov(train_preprocessed, use = "pairwise.complete.obs")
mahalanobis(data, train_mean, train_cov, inverted = TRUE)
}
## Calculate Mahalanobis Distance for Validation Set
train_distances <- mahalanobis_distance(train_preprocessed, train_preprocessed)
validation_distances <- mahalanobis_distance(validation_preprocessed, train_preprocessed)
upper_bound <- quantile(train_distances, probs = c(0.999), na.rm = T)
## Plot Distances
gg_maha_dist_input <- bind_rows(tibble(label = "Train", distance = train_distances), tibble(label = "Validation", distance = validation_distances))
gg_maha_dist <-
ggplot(gg_maha_dist_input |> dplyr::filter(label == "Train") |> slice_sample(prop = 0.01), aes(x = distance)) +
stat_ecdf() +
geom_vline(xintercept = upper_bound, linetype = "dashed") +
annotate("text", x = upper_bound, y = 0.5, label = "Out-of-Distribution", angle = -90, hjust = 0, vjust = -0.5, fontface = "bold") +
labs(title = "Mahalanobis Distance from Training Set",
x = "Mahalanobis Distance",
y = "Cumulative Proportion") +
scale_x_log10()
suppressWarnings(gg_maha_dist)
library(gt)
validation_preprocessed_with_distances <- validation_preprocessed |> mutate(mahalanobis_distance = validation_distances)
validation_preprocessed_with_distances |> arrange(desc(mahalanobis_distance)) |> slice_head(n = 1) |> select(-matches("delta|dist")) |> pivot_longer(cols = everything(), names_to = "Analyte", values_to = "Result") |> gt() |> tab_header("Example Out-of-Distribution BMP")
#| fig-width: 12
#| fig-height: 4
## Load package
library(applicable)
## Train PCA Model
train_pca <- apd_pca(train_preprocessed |> drop_na())
## Calculate Distance in PC space
pca_score <- score(train_pca, validation_preprocessed_with_distances)
## Add Distance to Validation Data
validation_preprocessed_with_distances <- validation_preprocessed_with_distances |> mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl)
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_distance)) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile") +
scale_y_log10(name = "Mahalanobis Distance") +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
validation_preprocessed_with_distances
validation |> bind_cols(validation_preprocessed_with_distances |> select(matches("dist"))) |> arrange(desc(mahalanobis_distance)) |> slice_head(n = 1) |> select(-matches("delta|dist")) |> pivot_longer(cols = everything(), names_to = "Analyte", values_to = "Result") |> gt() |> tab_header("Example Out-of-Distribution BMP")
validation |> bind_cols(validation_preprocessed_with_distances |> select(matches("dist"))) |> arrange(desc(mahalanobis_distance)) |> slice_head(n = 1)
validation |> bind_cols(validation_preprocessed_with_distances |> select(matches("dist"))) |> arrange(desc(mahalanobis_distance)) |> slice_head(n = 1) |> select(-matches("delta|dist|id|dt_tm")) |> pivot_longer(cols = everything(), names_to = "Analyte", values_to = "Result") |> gt() |> tab_header("Example Out-of-Distribution BMP")
validation |> bind_cols(validation_preprocessed_with_distances |> select(matches("dist"))) |> arrange(desc(mahalanobis_distance)) |> slice_head(n = 1) |> select(-matches("delta|dist|_id|dt_tm")) |> pivot_longer(cols = everything(), names_to = "Analyte", values_to = "Result") |> gt() |> tab_header("Example Out-of-Distribution BMP")
validation
validation |> bind_cols(validation_preprocessed_with_distances |> select(matches("dist"))) |> arrange(desc(mahalanobis_distance)) |> slice_head(n = 1)
validation |> select(sodium, chloride, potassium_plas, co2_totl, bun, creatinine, calcium, glucose) |> bind_cols(validation_preprocessed_with_distances |> select(matches("dist"))) |> arrange(desc(mahalanobis_distance)) |> slice_head(n = 1) |> select(-matches("delta|dist|_id|dt_tm")) |> pivot_longer(cols = everything(), names_to = "Analyte", values_to = "Result") |> gt() |> tab_header("Example Out-of-Distribution BMP")
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = quantile(mahalanobis_distance))) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile") +
scale_y_log10(name = "Mahalanobis Distance") +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = quantile(mahalanobis_distance, na.rm = T))) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile") +
scale_y_log10(name = "Mahalanobis Distance") +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
## Add Distance to Validation Data
validation_preprocessed_with_distances <- validation_preprocessed_with_distances |> mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = quantile(mahalanobis_distance, na.rm = T))
## Add Distance to Validation Data
validation_preprocessed_with_distances <- validation_preprocessed_with_distances |> mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = ntile(mahalanobis_distance, na.rm = T))
## Add Distance to Validation Data
validation_preprocessed_with_distances <- validation_preprocessed_with_distances |> mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = ntile(mahalanobis_distance))
## Add Distance to Validation Data
validation_preprocessed_with_distances <- validation_preprocessed_with_distances |> mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = ntile(mahalanobis_distance, n = 1000))
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = quantile(mahalanobis_distance, na.rm = T))) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile") +
scale_y_log10(name = "Mahalanobis Distance") +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_pctl)) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile") +
scale_y_log10(name = "Mahalanobis Distance") +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_pctl)) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile") +
scale_y_continuous(name = "Mahalanobis Percentile") +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
## Add Distance to Validation Data
validation_preprocessed_with_distances <- validation_preprocessed_with_distances |> mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = ntile(mahalanobis_distance, n = 100))
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_pctl)) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile") +
scale_y_continuous(name = "Mahalanobis Percentile") +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_pctl)) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile", breaks = c(0, 50, 100), labels = c(0, 50, 100)) +
scale_y_continuous(name = "Mahalanobis Percentile", breaks = c(0, 500, 1000), labels = c(0, 50, 100)) +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
## Add Distance to Validation Data
validation_preprocessed_with_distances <- validation_preprocessed_with_distances |> mutate(pca_distance = pca_score$distance, pca_pctl = pca_score$distance_pctl, mahalanobis_pctl = ntile(mahalanobis_distance, n = 1000))
## Plot PCA Distance as compared to Mahalanobis Distance
gg_pca_dist <-
ggplot(validation_preprocessed_with_distances, aes(x = pca_pctl, y = mahalanobis_pctl)) +
geom_point(alpha = 0.1, shape = ".") +
scale_x_continuous(name = "PCA Distance Percentile", breaks = c(0, 50, 100), labels = c(0, 50, 100)) +
scale_y_continuous(name = "Mahalanobis Percentile", breaks = c(0, 500, 1000), labels = c(0, 50, 100)) +
ggtitle("Comparison of Distance Metrics for Multivariate Applicability Assessment")
gg_pca_dist
renv::isolate()
renv::clean()
# Plot Results
ggplot(decision_curves, aes(x = .threshold, y = .estimate, color = .metric)) +
geomtextpath::geom_textline(aes(label = str_to_upper(.metric)), linewidth = 1.5, fontface = "bold", size = 8, hjust = 0.25) +
geom_vline(xintercept = max_mcc[[".threshold"]], linetype = "dashed") +
geom_text(data = max_mcc, x = max_mcc[[".threshold"]] - 0.02, y = 0.05, hjust = 1, label = glue::glue("Max MCC at a cut-off of ", max_mcc[[".threshold"]])) +
geom_point(data = max_mcc, size = 6, aes(color = "mcc")) +
geom_vline(xintercept = max_J[[".threshold"]], linetype = "dashed") +
geom_text(data = max_J, x = max_J[[".threshold"]] + 0.02, y = 0.05, hjust = 0, label = glue::glue("Max Youden's J at a cut-off of ", max_J[[".threshold"]])) +
geom_point(data = max_J, size = 6, aes(color = "j_index")) +
scico::scale_color_scico_d(palette = "lipari", begin = 0.1, end = 0.9) +
scale_x_continuous(name = "Prediction Threshold", breaks = c(0, 0.5, 1)) +
scale_y_continuous(name = "Metric Value", breaks = c(0, 0.5, 1)) +
ggtitle("Performance Metrics Across a Range of Thresholds") +
theme(legend.position = "none")
gg_dist_with_equiv <-
gg_dist +
geom_vline(xintercept = c(0.25, 0.75), linetype = "dashed") +
annotate("rect", xmin = 0.25, xmax = 0.75, ymin = 0, ymax = 5, fill = "gray70", alpha = 0.5) +
annotate("text", x = 0.4, y = 2.5, label = "Equivocal", hjust = 0.5, size = 6, fontface = "bold") +
## Add a segment with an arrow on either side
annotate("segment", x = 0.5, xend = 0.73, y = 2.5, yend = 2.5, arrow = arrow(type = "closed", length = unit(0.1, "inches"))) +
annotate("segment", x = 0.3, xend = 0.27, y = 2.5, yend = 2.5, arrow = arrow(type = "closed", length = unit(0.1, "inches"))) +
ggtitle("Decision Boundaries with an Equivocal Zone")
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
# Load Model and Data
## Load models
options(timeout=300)
model_realtime <- read_rds("https://figshare.com/ndownloader/files/45631488") |> pluck("model") |> bundle::unbundle()
recipe <- model_realtime |> extract_recipe()
predictors <- recipe$term_info |> dplyr::filter(role == "predictor") |> pluck("variable")
validation <- arrow::read_feather("https://figshare.com/ndownloader/files/45407398") |> select(any_of(predictors))
validation_with_predictions <- augment(model_realtime, validation |> drop_na(matches("delta_prior"))) |> slice_head(prop = 0.01, by = ".pred_class")
validation_preprocessed <- bake(recipe, validation_with_predictions) |> bind_cols(validation_with_predictions |> select(matches("pred")))
library(iml)
predict_wrapper <- function(model, newdata){workflows:::predict.workflow(object = model, new_data = newdata, type = "prob")}
predictor <- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |> select(any_of(predictors))), y = validation_with_predictions[[".pred_1"]], predict.function = predict_wrapper, type = "prob", class = 2)
pdp <- FeatureEffect$new(predictor, feature = "chloride", method = "pdp")
gg_pdp <- plot(pdp) + xlab("Chloride (mmol/L)") + scale_y_continuous(name = "Average Marginal Impact") + ggtitle("Partial Dependence Plot") + coord_cartesian(xlim = c(80, 140))
#| fig-width: 13
#| fig-height: 4
predictor <- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |> select(any_of(predictors))), y = validation_with_predictions[[".pred_1"]], predict.function = predict_wrapper, type = "prob", class = 2)
ale <- FeatureEffect$new(predictor, feature = "chloride")
gg_ale <- plot(ale) + scale_x_continuous(name = "Chloride (mmol/L)") +
scale_y_continuous(name = "Average Conditional Impact") +
scico::scale_fill_scico(palette = "lipari", begin = 0.1, end = 0.9, name = "Impact on Prediction") +
coord_cartesian(xlim = c(80, 140)) +
ggtitle("Accumulated Local Effects Plot")
ggpubr::ggarrange(gg_pdp, gg_ale, ncol = 2, nrow = 1)
# Pick a random highly positive example
local_example <- validation_with_predictions |> arrange(desc(.pred_1)) |> slice_head(n = 1)
# Rename columns without _delta_prior
deltas <- local_example |> select(matches("_delta_prior")) |> rename_all(~str_remove(.x, "_delta_prior"))
# Print a table of deltas and results
example_table <- as.data.frame(bind_rows(local_example |> select(any_of(predictors)) |> select(-matches("_delta_prior")), local_example |> select(any_of(predictors)) |> select(-matches("_delta_prior")) + deltas))
row.names(example_table) <- c("Prior", "Current")
knitr::kable(example_table, digits = 2, row.names = T)
#| fig-width: 12
#| fig-height: 6
# Load Libraries
library(shapviz)
# Build SHAP explainer
shap_local <- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(local_example |> select(any_of(predictors)) %>% bake(recipe, .)), X = local_example)
shap_local$S <- as.matrix(shap_local$S * -1)
# Plot SHAP Values Locally
sv_waterfall(shap_local, show_annotation = F) +
ggtitle("Local Explanation of a Positive Prediction with SHAP") +
scico::scale_fill_scico_d(palette = "vik", begin = 0.9, end = 0.1) +
theme(plot.title = element_text(size = 18, face = "bold.italic"))
#| fig-width: 12
#| fig-height: 6
#|
# Build SHAP explainer
shap <- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(validation_with_predictions |> select(any_of(predictors)) %>% bake(recipe, .)), X = as.data.frame(validation_with_predictions))
shap$S <- as.matrix(shap$S * -1)
# Plot SHAP Values as Beeswarm Plot
gg_bee <- sv_importance(shap, kind = "beeswarm", max_display = 5, alpha = 0.5) + scico::scale_color_scico(palette = "vik", breaks = c(0, 1), labels = c("Low", "High"), name = "Feature Value") + xlab("Impact on Prediction") + theme(legend.position = c(0.15, 0.15), legend.direction = "horizontal", legend.title.position = "top", axis.text.x.bottom = element_blank())
gg_bee
#| echo: false
ggsave("../../figures/shap_beeswarm.png", gg_bee, width = 5, height = 6, dpi = 1200)
ggsave("../../figures/shap_beeswarm.pdf", gg_bee, width = 5, height = 6)
ggsave("../../figures/shap_beeswarm.svg", gg_bee, width = 5, height = 6)
# Plot SHAP Values as Beeswarm Plot
gg_bee <- sv_importance(shap, kind = "beeswarm", max_display = 5, alpha = 0.5) + scico::scale_color_scico(palette = "vik", breaks = c(0, 1), labels = c("Low", "High"), name = "Feature Value") + xlab("Impact on Prediction") + theme(legend.position = c(0.85, 0.20), legend.direction = "horizontal", legend.title.position = "top", axis.text.x.bottom = element_blank())
gg_bee
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
# Load Model and Data
## Load models
options(timeout=300)
model_realtime <- read_rds("https://figshare.com/ndownloader/files/45631488") |> pluck("model") |> bundle::unbundle()
recipe <- model_realtime |> extract_recipe()
predictors <- recipe$term_info |> dplyr::filter(role == "predictor") |> pluck("variable")
validation <- arrow::read_feather("https://figshare.com/ndownloader/files/45407398") |> select(any_of(predictors))
validation_with_predictions <- augment(model_realtime, validation |> drop_na(matches("delta_prior"))) |> slice_head(prop = 0.10, by = ".pred_class")
validation_preprocessed <- bake(recipe, validation_with_predictions) |> bind_cols(validation_with_predictions |> select(matches("pred")))
library(iml)
predict_wrapper <- function(model, newdata){workflows:::predict.workflow(object = model, new_data = newdata, type = "prob")}
predictor <- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |> select(any_of(predictors))), y = validation_with_predictions[[".pred_1"]], predict.function = predict_wrapper, type = "prob", class = 2)
pdp <- FeatureEffect$new(predictor, feature = "chloride", method = "pdp")
gg_pdp <- plot(pdp) + xlab("Chloride (mmol/L)") + scale_y_continuous(name = "Average Marginal Impact") + ggtitle("Partial Dependence Plot") + coord_cartesian(xlim = c(80, 140))
#| fig-width: 13
#| fig-height: 4
predictor <- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |> select(any_of(predictors))), y = validation_with_predictions[[".pred_1"]], predict.function = predict_wrapper, type = "prob", class = 2)
ale <- FeatureEffect$new(predictor, feature = "chloride")
gg_ale <- plot(ale) + scale_x_continuous(name = "Chloride (mmol/L)") +
scale_y_continuous(name = "Average Conditional Impact") +
scico::scale_fill_scico(palette = "lipari", begin = 0.1, end = 0.9, name = "Impact on Prediction") +
coord_cartesian(xlim = c(80, 140)) +
ggtitle("Accumulated Local Effects Plot")
ggpubr::ggarrange(gg_pdp, gg_ale, ncol = 2, nrow = 1)
# Pick a random highly positive example
local_example <- validation_with_predictions |> arrange(desc(.pred_1)) |> slice_head(n = 1)
# Rename columns without _delta_prior
deltas <- local_example |> select(matches("_delta_prior")) |> rename_all(~str_remove(.x, "_delta_prior"))
# Print a table of deltas and results
example_table <- as.data.frame(bind_rows(local_example |> select(any_of(predictors)) |> select(-matches("_delta_prior")), local_example |> select(any_of(predictors)) |> select(-matches("_delta_prior")) + deltas))
row.names(example_table) <- c("Prior", "Current")
knitr::kable(example_table, digits = 2, row.names = T)
#| fig-width: 12
#| fig-height: 6
# Load Libraries
library(shapviz)
# Build SHAP explainer
shap_local <- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(local_example |> select(any_of(predictors)) %>% bake(recipe, .)), X = local_example)
shap_local$S <- as.matrix(shap_local$S * -1)
# Plot SHAP Values Locally
sv_waterfall(shap_local, show_annotation = F) +
ggtitle("Local Explanation of a Positive Prediction with SHAP") +
scico::scale_fill_scico_d(palette = "vik", begin = 0.9, end = 0.1) +
theme(plot.title = element_text(size = 18, face = "bold.italic"))
#| fig-width: 13
#| fig-height: 6
#|
# Build SHAP explainer
shap <- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(validation_with_predictions |> select(any_of(predictors)) %>% bake(recipe, .)), X = as.data.frame(validation_with_predictions))
shap$S <- as.matrix(shap$S * -1)
# Plot SHAP Values as Beeswarm Plot
gg_bee <- sv_importance(shap, kind = "beeswarm", max_display = 5, alpha = 0.5) + scico::scale_color_scico(palette = "vik", breaks = c(0, 1), labels = c("Low", "High"), name = "Feature Value") + xlab("Impact on Prediction") + theme(legend.position = c(0.85, 0.20), legend.direction = "horizontal", legend.title.position = "top", axis.text.x.bottom = element_blank())
gg_bee
#| echo: false
ggsave("../../figures/shap_beeswarm.png", gg_bee, width = 5, height = 6, dpi = 1200)
ggsave("../../figures/shap_beeswarm.pdf", gg_bee, width = 5, height = 6)
ggsave("../../figures/shap_beeswarm.svg", gg_bee, width = 5, height = 6)
# Plot SHAP Values as Beeswarm Plot
gg_bee <- sv_importance(shap, kind = "beeswarm", max_display = 5, alpha = 0.25) + scico::scale_color_scico(palette = "vik", breaks = c(0, 1), labels = c("Low", "High"), name = "Feature Value") + xlab("Impact on Prediction") + theme(legend.position = c(0.85, 0.20), legend.direction = "horizontal", legend.title.position = "top", axis.text.x.bottom = element_blank())
gg_bee
validation_with_predictions <- augment(model_realtime, validation |> drop_na(matches("delta_prior"))) |> slice_head(prop = 0.05, by = ".pred_class")
validation_with_predictions <- augment(model_realtime, validation |> drop_na(matches("delta_prior"))) |> slice_head(prop = 0.01, by = ".pred_class")
validation_with_predictions <- augment(model_realtime, validation |> drop_na(matches("delta_prior"))) |> slice_head(prop = 0.05, by = ".pred_class")
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
# Load Model and Data
## Load models
options(timeout=300)
model_realtime <- read_rds("https://figshare.com/ndownloader/files/45631488") |> pluck("model") |> bundle::unbundle()
recipe <- model_realtime |> extract_recipe()
predictors <- recipe$term_info |> dplyr::filter(role == "predictor") |> pluck("variable")
validation <- arrow::read_feather("https://figshare.com/ndownloader/files/45407398") |> select(any_of(predictors))
validation_with_predictions <- augment(model_realtime, validation |> drop_na(matches("delta_prior"))) |> slice_head(prop = 0.05, by = ".pred_class")
validation_preprocessed <- bake(recipe, validation_with_predictions) |> bind_cols(validation_with_predictions |> select(matches("pred")))
library(iml)
predict_wrapper <- function(model, newdata){workflows:::predict.workflow(object = model, new_data = newdata, type = "prob")}
predictor <- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |> select(any_of(predictors))), y = validation_with_predictions[[".pred_1"]], predict.function = predict_wrapper, type = "prob", class = 2)
pdp <- FeatureEffect$new(predictor, feature = "chloride", method = "pdp")
gg_pdp <- plot(pdp) + xlab("Chloride (mmol/L)") + scale_y_continuous(name = "Average Marginal Impact") + ggtitle("Partial Dependence Plot") + coord_cartesian(xlim = c(80, 140))
#| fig-width: 13
#| fig-height: 4
predictor <- Predictor$new(model = model_realtime, data = as.data.frame(validation_with_predictions |> select(any_of(predictors))), y = validation_with_predictions[[".pred_1"]], predict.function = predict_wrapper, type = "prob", class = 2)
ale <- FeatureEffect$new(predictor, feature = "chloride")
gg_ale <- plot(ale) + scale_x_continuous(name = "Chloride (mmol/L)") +
scale_y_continuous(name = "Average Conditional Impact") +
scico::scale_fill_scico(palette = "lipari", begin = 0.1, end = 0.9, name = "Impact on Prediction") +
coord_cartesian(xlim = c(80, 140)) +
ggtitle("Accumulated Local Effects Plot")
ggpubr::ggarrange(gg_pdp, gg_ale, ncol = 2, nrow = 1)
# Pick a random highly positive example
local_example <- validation_with_predictions |> arrange(desc(.pred_1)) |> slice_head(n = 1)
# Rename columns without _delta_prior
deltas <- local_example |> select(matches("_delta_prior")) |> rename_all(~str_remove(.x, "_delta_prior"))
# Print a table of deltas and results
example_table <- as.data.frame(bind_rows(local_example |> select(any_of(predictors)) |> select(-matches("_delta_prior")), local_example |> select(any_of(predictors)) |> select(-matches("_delta_prior")) + deltas))
row.names(example_table) <- c("Prior", "Current")
knitr::kable(example_table, digits = 2, row.names = T)
#| fig-width: 12
#| fig-height: 6
# Load Libraries
library(shapviz)
# Build SHAP explainer
shap_local <- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(local_example |> select(any_of(predictors)) %>% bake(recipe, .)), X = local_example)
shap_local$S <- as.matrix(shap_local$S * -1)
# Plot SHAP Values Locally
sv_waterfall(shap_local, show_annotation = F) +
ggtitle("Local Explanation of a Positive Prediction with SHAP") +
scico::scale_fill_scico_d(palette = "vik", begin = 0.9, end = 0.1) +
theme(plot.title = element_text(size = 18, face = "bold.italic"))
#| fig-width: 13
#| fig-height: 6
#|
# Build SHAP explainer
shap <- shapviz(extract_fit_engine(model_realtime), X_pred = as.matrix(validation_with_predictions |> select(any_of(predictors)) %>% bake(recipe, .)), X = as.data.frame(validation_with_predictions))
shap$S <- as.matrix(shap$S * -1)
# Plot SHAP Values as Beeswarm Plot
gg_bee <- sv_importance(shap, kind = "beeswarm", max_display = 5, alpha = 0.25) + scico::scale_color_scico(palette = "vik", breaks = c(0, 1), labels = c("Low", "High"), name = "Feature Value") + xlab("Impact on Prediction") + theme(legend.position = c(0.85, 0.20), legend.direction = "horizontal", legend.title.position = "top", axis.text.x.bottom = element_blank())
gg_bee
#| echo: false
ggsave("../../figures/shap_beeswarm.png", gg_bee, width = 5, height = 6, dpi = 1200)
ggsave("../../figures/shap_beeswarm.pdf", gg_bee, width = 5, height = 6)
ggsave("../../figures/shap_beeswarm.svg", gg_bee, width = 5, height = 6)
# Plot SHAP Values as Beeswarm Plot
gg_bee <- sv_importance(shap, kind = "beeswarm", max_display = 5, alpha = 0.75) + scico::scale_color_scico(palette = "vik", breaks = c(0, 1), labels = c("Low", "High"), name = "Feature Value") + xlab("Impact on Prediction") + theme(legend.position = c(0.85, 0.20), legend.direction = "horizontal", legend.title.position = "top", axis.text.x.bottom = element_blank())
gg_bee
library(tidymodels)
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401")
renv::activate()
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401")
renv::install("arrow")
library(tidymodels)
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401")
renv::repair("arrow")
renv::install("arrow")
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401")
library(tidymodels)
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401")
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401")
train
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401") |> select(target, bun:sodium)
train
train |> count(unlikely_comment, contam_comment)
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401") |> select(contam_comment, bun:sodium)
train
# Define the feature recipe
recipe <- recipe(contam_comment ~ ., data = train)
# Define the model
model <- boost_tree() |> set_engine("xgboost")
# Create the workflow
workflow <- workflow() |> add_recipe(recipe)
# Create the workflow
workflow <- workflow() |> add_recipe(recipe) |> add_model(model)
# Define the model
model <- boost_tree(mode = "classification") |> set_engine("xgboost")
model
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401") |> select(contam_comment, bun:sodium)
# Define the feature recipe
recipe <- recipe(contam_comment ~ ., data = train)
# Define the model
model <- boost_tree(mode = "classification") |> set_engine("xgboost")
# Create the workflow
workflow <- workflow() |> add_recipe(recipe) |> add_model(model)
# Fit the model
fit <- workflow |> fit(data = train)
train |> transmute(bun:sodium, target = factor(contam_comment))
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401") |> select(contam_comment, bun:sodium) |> mutate(target = factor(contam_comment, labels = c("Real", "Contaminated")))
# Define the feature recipe
recipe <- recipe(contam_comment ~ ., data = train)
# Define the model
model <- boost_tree(mode = "classification") |> set_engine("xgboost")
# Define the feature recipe
recipe <- recipe(contam_comment ~ ., data = train)
# Define the model
model <- boost_tree(mode = "classification") |> set_engine("xgboost")
# Create the workflow
workflow <- workflow() |> add_recipe(recipe) |> add_model(model)
# Fit the model
fit <- workflow |> fit(data = train)
# Load the data
train <- arrow::read_feather("https://figshare.com/ndownloader/files/45407401") |> select(contam_comment, bun:sodium) |>
mutate(contam_comment = factor(contam_comment, labels = c("Real", "Contaminated")))
# Define the feature recipe
recipe <- recipe(contam_comment ~ ., data = train)
# Define the model
model <- boost_tree(mode = "classification") |> set_engine("xgboost")
# Create the workflow
workflow <- workflow() |> add_recipe(recipe) |> add_model(model)
# Fit the model
fit <- workflow |> fit(data = train)
fit
fit
renv::hydrate()
renv::install("yaml")
renv::install("rlang")
renv::update()
renv::record("renv@1.1.4")
renv::install("tidyverse")
renv::status()
?renv::status()
renv::restore()
